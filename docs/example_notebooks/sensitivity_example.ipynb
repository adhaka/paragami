{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paragami\n",
    "\n",
    "import autograd\n",
    "from autograd import numpy as np\n",
    "import copy\n",
    "\n",
    "# Use the original scipy for functions we don't need to differentiate.\n",
    "import scipy as osp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_obs = 500\n",
    "data_dim = 3\n",
    "\n",
    "# True values of parameters\n",
    "true_sigma = \\\n",
    "    np.eye(3) * np.diag(np.arange(0, data_dim)) + \\\n",
    "    np.random.random((data_dim, data_dim)) * 0.1\n",
    "true_sigma = 0.5 * (true_sigma + true_sigma.T)\n",
    "true_mu = np.arange(0, data_dim)\n",
    "\n",
    "true_norm_param_dict = dict()\n",
    "true_norm_param_dict['mu'] = true_mu\n",
    "true_norm_param_dict['sigma'] = true_sigma\n",
    "\n",
    "# Data\n",
    "data = np.random.multivariate_normal(\n",
    "    mean=true_norm_param_dict['mu'],\n",
    "    cov=true_norm_param_dict['sigma'],\n",
    "    size=(num_obs, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at true parameter: 69.74233683848566\n"
     ]
    }
   ],
   "source": [
    "def get_mvn_log_probs(obs, mean, cov):\n",
    "    cov_inv = np.linalg.inv(cov)\n",
    "    cov_det_sign, cov_log_det = np.linalg.slogdet(cov)\n",
    "    if cov_det_sign <= 0:\n",
    "        return np.full(float('inf'), obs.shape[0])\n",
    "    else:\n",
    "        obs_centered = obs - np.expand_dims(mean, axis=0)\n",
    "        return -0.5 * (\n",
    "            np.einsum('ni,ij,nj->n', obs_centered, cov_inv, obs_centered) + \\\n",
    "            cov_log_det)\n",
    "\n",
    "def get_data_lp(data, norm_param_dict, weights):\n",
    "    data_lp = np.sum(weights *\n",
    "                     get_mvn_log_probs(\n",
    "                         data,\n",
    "                         mean=norm_param_dict['mu'],\n",
    "                         cov=norm_param_dict['sigma']))\n",
    "    return data_lp\n",
    "\n",
    "def get_prior_lp(norm_param_dict, prior_param_dict):\n",
    "    data_dim = len(prior_param_dict['prior_mean']) \n",
    "    prior_cov = np.eye(data_dim) * (prior_param_dict['prior_sd'] ** 2)\n",
    "    prior_lp = get_mvn_log_probs(\n",
    "        obs=np.expand_dims(norm_param_dict['mu'], axis=0),\n",
    "        mean=prior_param_dict['prior_mean'],\n",
    "        cov=prior_cov)\n",
    "\n",
    "    # Sum so as to return a scalar.\n",
    "    return np.sum(prior_lp)\n",
    "\n",
    "def get_loss(data, norm_param_dict, prior_param_dict, weights):\n",
    "    return -1 * (get_prior_lp(norm_param_dict, prior_param_dict) +\n",
    "                 get_data_lp(data, norm_param_dict, weights))\n",
    "    \n",
    "class NormalModel():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.num_obs = self.data.shape[0]\n",
    "        self.data_dim = self.data.shape[1]\n",
    "                \n",
    "        # Reasonable defaults for the priors and weights.\n",
    "        self.set_prior(np.full(self.data_dim, 0.), 10)\n",
    "        self.set_weights(np.full(self.num_obs, 1.0))\n",
    "                \n",
    "    def set_weights(self, weights):\n",
    "        self.weights = weights\n",
    "    \n",
    "    def set_prior(self, prior_mean, prior_sd):\n",
    "        self.prior_dict = dict()\n",
    "        self.prior_dict['prior_mean'] = prior_mean\n",
    "        self.prior_dict['prior_sd'] = prior_sd\n",
    "\n",
    "    def get_loss_for_opt(self, norm_param_dict):\n",
    "        return get_loss(\n",
    "            self.data, norm_param_dict, self.prior_dict, self.weights)\n",
    "    \n",
    "    def get_loss_by_prior(self, norm_param_dict, prior_dict):\n",
    "        return get_loss(\n",
    "            self.data, norm_param_dict, prior_dict, self.weights)\n",
    "\n",
    "    def get_loss_by_weights(self, norm_param_dict, weights):\n",
    "        return get_loss(\n",
    "            self.data, norm_param_dict, self.prior_dict, weights)\n",
    "\n",
    "    \n",
    "model = NormalModel(data)\n",
    "orig_prior_dict = copy.deepcopy(model.prior_dict)\n",
    "orig_weights = copy.deepcopy(model.weights)\n",
    "print('Loss at true parameter: {}'.format(model.get_loss_for_opt(true_norm_param_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_pattern = paragami.PatternDict()\n",
    "norm_pattern['mu'] = paragami.NumericArrayPattern(shape=(data_dim, ))\n",
    "norm_pattern['sigma'] = paragami.PSDMatrixPattern(size=data_dim)\n",
    "\n",
    "prior_pattern = paragami.PatternDict()\n",
    "prior_pattern['prior_mean'] = paragami.NumericArrayPattern(shape=(data_dim, ))\n",
    "prior_pattern['prior_sd'] = paragami.NumericArrayPattern(shape=(1, ), lb=0.0)\n",
    "\n",
    "weight_pattern = paragami.NumericArrayPattern(shape=(num_obs, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 65.200672\n",
      "         Iterations: 18\n",
      "         Function evaluations: 20\n",
      "         Gradient evaluations: 17\n",
      "         Hessian evaluations: 17\n"
     ]
    }
   ],
   "source": [
    "# Optimize.\n",
    "opt_fun = paragami.FlattenedFunction(\n",
    "    original_fun=model.get_loss_for_opt,\n",
    "    patterns=norm_pattern,\n",
    "    free=True)\n",
    "opt_fun_grad = autograd.grad(opt_fun)\n",
    "opt_fun_hessian = autograd.hessian(opt_fun)\n",
    "\n",
    "def get_optimum(init_param):\n",
    "    return osp.optimize.minimize(\n",
    "        method='trust-ncg',\n",
    "        x0=init_param,\n",
    "        fun=opt_fun,\n",
    "        jac=opt_fun_grad,\n",
    "        hess=opt_fun_hessian,\n",
    "        options={'gtol': 1e-8, 'disp': True})\n",
    "\n",
    "# Initialize with zeros.\n",
    "init_param = np.zeros(norm_pattern.flat_length(free=True))\n",
    "mle_opt = get_optimum(init_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('mu', array([0.00837093, 0.95353189, 1.89852799])), ('sigma', array([[0.03865698, 0.08549516, 0.03813797],\n",
      "       [0.08549516, 1.05532368, 0.09781193],\n",
      "       [0.03813797, 0.09781193, 1.91488823]]))])\n",
      "{'sigma': array([[0.03745401, 0.07746864, 0.03950388],\n",
      "       [0.07746864, 1.01560186, 0.05110853],\n",
      "       [0.03950388, 0.05110853, 2.0601115 ]]), 'mu': array([0, 1, 2])}\n"
     ]
    }
   ],
   "source": [
    "opt_norm_param_dict = norm_pattern.fold(mle_opt.x, free=True)\n",
    "print(opt_norm_param_dict)\n",
    "print(true_norm_param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.prior_dict = orig_prior_dict\n",
    "model.weights = orig_weights\n",
    "prior_sens = \\\n",
    "    paragami.HyperparameterSensitivityLinearApproximation(\n",
    "        objective_fun=model.get_loss_by_prior,\n",
    "        opt_par_pattern=norm_pattern,\n",
    "        hyper_par_pattern=prior_pattern,\n",
    "        opt_par_folded_value=opt_norm_param_dict,\n",
    "        hyper_par_folded_value=orig_prior_dict,\n",
    "        validate_optimum=False,\n",
    "        opt_par_is_free=True,\n",
    "        hyper_par_is_free=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This helper function lets us easily see the differences in parameters.\n",
    "def get_norm_param_diff(par1, par2):\n",
    "    diff = \\\n",
    "        norm_pattern.flatten(par1, free=False, validate=False) - \\\n",
    "        norm_pattern.flatten(par2, free=False, validate=False)\n",
    "    return norm_pattern.fold(diff, free=False, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 212.336790\n",
      "         Iterations: 6\n",
      "         Function evaluations: 8\n",
      "         Gradient evaluations: 7\n",
      "         Hessian evaluations: 7\n"
     ]
    }
   ],
   "source": [
    "# Change the prior.\n",
    "new_prior_dict = copy.deepcopy(orig_prior_dict)\n",
    "change_mean = True\n",
    "\n",
    "# Note: it does not seem to be working for the prior cov.  Is this\n",
    "# real nonlinearity or a bug?\n",
    "change_cov = False\n",
    "if change_mean:\n",
    "    new_prior_dict['prior_mean'] = orig_prior_dict['prior_mean'] + 100\n",
    "if change_cov:\n",
    "    new_prior_dict['prior_sd'] = 0.1 * orig_prior_dict['prior_sd']\n",
    "\n",
    "# Make sure we use the original weights.\n",
    "model.weights = orig_weights\n",
    "\n",
    "# Get the linear prediction at the new prior.\n",
    "pred_norm_param_dict = \\\n",
    "    prior_sens.predict_opt_par_from_hyper_par(new_prior_dict)\n",
    "\n",
    "# Re-optimize to check the prior sensitivity.\n",
    "model.prior_dict = new_prior_dict\n",
    "new_opt_par = get_optimum(norm_pattern.flatten(opt_norm_param_dict, free=True))\n",
    "new_norm_param_dict = norm_pattern.fold(new_opt_par.x, free=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted mu differences:\n",
      "[0.00032457 0.0024772  0.00410151]\n",
      "Actual mu differences:\n",
      "[0.00032458 0.00247723 0.00410157]\n",
      "Predicted sigma differences:\n",
      "[[-2.00263363e-09 -1.53845964e-08 -3.68601111e-08]\n",
      " [-1.53845964e-08 -1.18182265e-07 -2.82588523e-07]\n",
      " [-3.68601111e-08 -2.82588523e-07 -6.11786368e-07]]\n",
      "Actual sigma differences:\n",
      "[[1.03350001e-07 7.88671136e-07 1.29453400e-06]\n",
      " [7.88671136e-07 6.01848912e-06 9.87918323e-06]\n",
      " [1.29453400e-06 9.87918323e-06 1.62113809e-05]]\n"
     ]
    }
   ],
   "source": [
    "# Look at the differences.\n",
    "pred_diff = get_norm_param_diff(\n",
    "    pred_norm_param_dict, opt_norm_param_dict)\n",
    "true_diff = get_norm_param_diff(\n",
    "    new_norm_param_dict, opt_norm_param_dict)\n",
    "for param in ['mu', 'sigma']:\n",
    "    print('Predicted {} differences:\\n{}'.format(param, pred_diff[param]))\n",
    "    print('Actual {} differences:\\n{}'.format(param, true_diff[param]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sensitivity to data weights.\n",
    "model.prior_dict = orig_prior_dict\n",
    "model.weights = orig_weights\n",
    "weight_sens = \\\n",
    "    paragami.HyperparameterSensitivityLinearApproximation(\n",
    "        objective_fun=model.get_loss_by_weights,\n",
    "        opt_par_pattern=norm_pattern,\n",
    "        hyper_par_pattern=weight_pattern,\n",
    "        opt_par_folded_value=opt_norm_param_dict,\n",
    "        hyper_par_folded_value=orig_weights,\n",
    "        validate_optimum=False,\n",
    "        opt_par_is_free=True,\n",
    "        hyper_par_is_free=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 212.336790\n",
      "         Iterations: 6\n",
      "         Function evaluations: 8\n",
      "         Gradient evaluations: 7\n",
      "         Hessian evaluations: 7\n"
     ]
    }
   ],
   "source": [
    "# Change the weights.\n",
    "obs = 5\n",
    "new_weights = np.ones(num_obs)\n",
    "new_weights[obs] = 0.0\n",
    "\n",
    "# Make sure we use the original prior.\n",
    "model.prior_dict = orig_prior_dict\n",
    "\n",
    "# Get the linear prediction at the new prior.\n",
    "pred_norm_param_dict = \\\n",
    "    weight_sens.predict_opt_par_from_hyper_par(new_weights)\n",
    "\n",
    "# Re-optimize to check the prior sensitivity.\n",
    "model.weights = new_weights\n",
    "new_opt_par = get_optimum(norm_pattern.flatten(opt_norm_param_dict, free=True))\n",
    "new_norm_param_dict = norm_pattern.fold(new_opt_par.x, free=True)\n",
    "\n",
    "# Look at the differences.\n",
    "pred_diff = get_norm_param_diff(pred_norm_param_dict, opt_norm_param_dict)\n",
    "true_diff = get_norm_param_diff(new_norm_param_dict, opt_norm_param_dict)\n",
    "for param in ['mu', 'sigma']:\n",
    "    print('Predicted {} differences:\\n{}'.format(param, pred_diff[param]))\n",
    "    print('Actual {} differences:\\n{}'.format(param, true_diff[param]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
