{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import paragami\n",
    "\n",
    "import autograd\n",
    "from autograd import numpy as np\n",
    "from autograd import scipy as sp\n",
    "\n",
    "# Use the original scipy for functions we don't need to differentiate.\n",
    "import scipy as osp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `paragami` with a positive semi-definite matrix parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by considering a simple symmetric positive semi-definite matrix.  In general, we can write:\n",
    "\n",
    "$$\n",
    "A = \\left[\n",
    "\\begin{matrix}\n",
    "a_{11} & a_{12} & a_{13}  \\\\\n",
    "a_{21} & a_{22} & a_{23}  \\\\\n",
    "a_{31} & a_{32} & a_{33}  \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "...although, of course, symmetry and positive semi-definiteness impose constraints on the entries $a_{ij}$ of $A$ which we will discuss below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening and folding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first consider how to represent $A$ as a vector, and then as an unconstrained vector.\n",
    "\n",
    "Because $A$ is symmetric, $a_{21} = a_{12}$, $a_{31} = a_{13}$, and $a_{23} = a_{32}$.  This means that to reproduce $A$ we only need to store the bold values:\n",
    "\n",
    "$$\n",
    "A = \\left[\n",
    "\\begin{matrix}\n",
    "\\mathbf{a_{11}} & \\mathbf{a_{12}} & \\mathbf{a_{13}}  \\\\\n",
    "a_{21} & \\mathbf{a_{22}} & \\mathbf{a_{23}}  \\\\\n",
    "a_{31} & a_{32} & \\mathbf{a_{33}}  \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "If we wanted to represent $A$ as an flat array, we could write\n",
    "\n",
    "$$\n",
    "A_{flat} = \\left[\n",
    "\\begin{matrix}\n",
    "\\mathbf{a_{11}} \\\\\n",
    "\\mathbf{a_{12}} \\\\\n",
    "\\mathbf{a_{22}} \\\\\n",
    "\\mathbf{a_{13}} \\\\\n",
    "\\mathbf{a_{23}} \\\\\n",
    "\\mathbf{a_{33}} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "and fully recover $A$ from $A_{flat}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to and from $A$ and $A_{flat}$ can be done with the `flatten` method of a `paragami.PSDMatrixPattern` pattern.  \n",
    "\n",
    "For the moment, because we are simply re-stacking the values of $A$ into a vector, not transforming into an unconstrained space, we use the option `free=False`.  We will discuss the `free=True` option shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, a_flat contains the elements of a exactly as shown in the formula above.\n",
      "\n",
      "a:\n",
      "[[1.43029514 0.91026842 0.28169565]\n",
      " [0.91026842 1.96292882 0.67954451]\n",
      " [0.28169565 0.67954451 1.39334116]]\n",
      "\n",
      "a_flat:\n",
      "[1.43029514 0.91026842 1.96292882 0.28169565 0.67954451 1.39334116]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A sample positive semi-definite matrix.\n",
    "a = np.eye(3) + np.random.random((3, 3))\n",
    "a = 0.5 * (a + a.T)\n",
    "\n",
    "# Define a pattern and fold.\n",
    "a_pattern = paragami.PSDMatrixPattern(size=3)\n",
    "a_flat = a_pattern.flatten(a, free=False)\n",
    "\n",
    "print('Now, a_flat contains the elements of a exactly as shown in the formula above.\\n')\n",
    "print('a:\\n{}\\n'.format(a))\n",
    "print('a_flat:\\n{}\\n'.format(a_flat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also convert from $A_{flat}$ back to $A$ by 'folding'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folding the flattened value recovers the original matrix.\n",
      "\n",
      "a:\n",
      "[[1.43029514 0.91026842 0.28169565]\n",
      " [0.91026842 1.96292882 0.67954451]\n",
      " [0.28169565 0.67954451 1.39334116]]\n",
      "\n",
      "a_fold:\n",
      "[1.43029514 0.91026842 1.96292882 0.28169565 0.67954451 1.39334116]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Folding the flattened value recovers the original matrix.\\n')\n",
    "a_fold = a_pattern.fold(a_flat, free=False)\n",
    "print('a:\\n{}\\n'.format(a))\n",
    "print('a_fold:\\n{}\\n'.format(a_flat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free flattening and folding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not every length-six vector represents a valid positive semi-definite matrix.  If the attribute `validate` of a `paragami.PSDMatrixPattern` is true, then folding automatically checks for validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The diagonal of a positive semi-definite matrix must not be less than 0, and folding checks this when validate=True, which it is by default:\n",
      "\n",
      "A bad folded value: [-1  0  0  0  0  0]\n",
      "Folding with a_pattern raised the following ValueError:\n",
      "Diagonal is less than the lower bound 0.0.\n",
      "\n",
      "If validate is false, folding will produce an invalid matrix without an error:\n",
      "\n",
      "Folding a non-pd matrix with validate=False:\n",
      "[[-1.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "\n",
      "However, it will not produce a matrix of the wrong shape even when validate is False:\n",
      "\n",
      "A very bad folded value: [1 0 0].\n",
      "Folding with a_pattern raised the following ValueError:\n",
      "Wrong length for PDMatrix flat value.\n"
     ]
    }
   ],
   "source": [
    "print('The diagonal of a positive semi-definite matrix must not be less',\n",
    "      'than 0, and folding checks this when validate=True, which it is by default:\\n')\n",
    "a_flat_bad = np.array([-1, 0, 0, 0, 0, 0])\n",
    "print('A bad folded value: {}'.format(a_flat_bad))\n",
    "try:\n",
    "    a_fold_bad = a_pattern.fold(a_flat_bad, free=False)\n",
    "except ValueError as err:\n",
    "    print('Folding with a_pattern raised the following ValueError:\\n{}'.format(err))\n",
    "\n",
    "print('\\nIf validate is false, folding will produce an invalid matrix without an error:\\n')\n",
    "a_pattern.validate = False\n",
    "a_fold_bad = a_pattern.fold(a_flat_bad, free=False)\n",
    "print('Folding a non-pd matrix with validate=False:\\n{}'.format(a_fold_bad))\n",
    "\n",
    "print('\\nHowever, it will not produce a matrix of the wrong shape even when validate is False:\\n')\n",
    "a_flat_very_bad = np.array([1, 0, 0])\n",
    "print('A very bad folded value: {}.'.format(a_flat_very_bad))\n",
    "try:\n",
    "    a_fold_very_bad = a_pattern.fold(a_flat_very_bad, free=False)\n",
    "except ValueError as err:\n",
    "    print('Folding with a_pattern raised the following ValueError:\\n{}'.format(err))\n",
    "\n",
    "# Let's set validate back to true.\n",
    "a_pattern.validate = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Sometimes (e.g. when performing optimization) it is convenient to have a flattened value that can legally take any value of the correct length.  For a positive semi-definite matrix, this can be achieved by a Cholesky decomposition.  The details aren't important here, as `paragami` takes care of the transformation behind the scenes with the option `free=True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The free flat value a_freeflat is not immediately recognizable as a.\n",
      "\n",
      "a:\n",
      "[[1.43029514 0.91026842 0.28169565]\n",
      " [0.91026842 1.96292882 0.67954451]\n",
      " [0.28169565 0.67954451 1.39334116]]\n",
      "\n",
      "a_freeflat:\n",
      "[0.17894041 0.76112615 0.16235011 0.23554143 0.42529939 0.07290737]\n",
      "\n",
      "However, it transforms correctly back to a when folded.\n",
      "\n",
      "a_fold:\n",
      "[[1.43029514 0.91026842 0.28169565]\n",
      " [0.91026842 1.96292882 0.67954451]\n",
      " [0.28169565 0.67954451 1.39334116]]\n",
      "\n",
      "Any length-six vector will free fold back to a valid positive semi-definite matrix up to floating point error. Let's draw 100 random vectors, fold them, and check that this is true.\n",
      "\n",
      "Note that you will get an incorrect values or errors if you mix free and non-free folding and flattening! \n",
      "In general, validate cannot be relied upon to catch such errors.\n",
      "\n",
      "a_fold_free_unfree_mix:\n",
      "[[-1.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('The free flat value a_freeflat is not immediately recognizable as a.\\n')\n",
    "a_freeflat = a_pattern.flatten(a, free=True)\n",
    "print('a:\\n{}\\n'.format(a))\n",
    "print('a_freeflat:\\n{}\\n'.format(a_freeflat))\n",
    "\n",
    "print('However, it transforms correctly back to a when folded.\\n')\n",
    "a_freefold = a_pattern.fold(a_freeflat, free=True)\n",
    "print('a_fold:\\n{}\\n'.format(a_freefold))\n",
    "\n",
    "print('Any length-six vector will free fold back to a valid positive',\n",
    "      'semi-definite matrix up to floating point error.',\n",
    "      'Let\\'s draw 100 random vectors, fold them, and check that this is true.\\n')\n",
    "# Draw random free vectors and confirm that they are positive semi definite.\n",
    "def assert_is_pd(mat):\n",
    "    eigvals = np.linalg.eigvals(mat)\n",
    "    assert np.min(eigvals) >= -1e-8\n",
    "for raw in range(100):\n",
    "    a_rand_freeflat = np.random.normal(scale=2, size=(6, ))\n",
    "    a_rand_fold = a_pattern.fold(a_rand_freeflat, free=True)\n",
    "    assert_is_pd(a_rand_fold)\n",
    "\n",
    "\n",
    "print('Note that you will get an incorrect values or errors if you mix',\n",
    "      'free and non-free folding and flattening!',\n",
    "      '\\nIn general, validate cannot be relied upon to catch such errors.\\n')\n",
    "a_pattern.validate = False\n",
    "a_fold_free_unfree_mix = a_pattern.fold(a_freeflat, free=False)\n",
    "print('a_fold_free_unfree_mix:\\n{}\\n'.format(a_fold_bad))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using flattening and folding for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are interested in optimizing some function of $A$, say, a normal model in which the data $x_n \\sim \\mathcal{N}(0, A)$.  Specifically, Let the data be $X = \\left(x_1, ..., x_N\\right)$, where $x_n \\in \\mathbb{R}^3$, and write a loss function as\n",
    "\n",
    "$$\n",
    "\\ell\\left(X, A\\right) =\n",
    "    -\\sum_{n=1}^N \\log P(x_n | A) =\n",
    "    \\frac{1}{2}\\sum_{n=1}^N \\left(x_n^T A^{-1} x_n - \\log|A|\\right) \n",
    "$$\n",
    "\n",
    "Let's simulate some data under this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1000, 3)\n",
      "Loss at true parameter: 2394.604318307261\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "N = 1000\n",
    "\n",
    "# True value of A\n",
    "true_a = np.eye(3) * np.diag(np.array([1, 2, 3])) + np.random.random((3, 3)) * 0.1\n",
    "true_a = 0.5 * (true_a + true_a.T)\n",
    "\n",
    "# Data\n",
    "def draw_data(N, true_a):\n",
    "    return np.random.multivariate_normal(\n",
    "        mean=np.zeros(3), cov=true_a, size=(N, ))\n",
    "\n",
    "x = draw_data(N, true_a)\n",
    "print('X shape: {}'.format(x.shape))\n",
    "\n",
    "# Later it will be convenient to separate the loss into a vector of\n",
    "# per-observation losses and a regularization term.\n",
    "def get_loss_by_n(x, a):\n",
    "    a_inv = np.linalg.inv(a)\n",
    "    a_det_sign, a_log_det = np.linalg.slogdet(a)\n",
    "    #assert a_det_sign > 0\n",
    "    return 0.5 * (np.einsum('ni,ij,nj->n', x, a_inv, x) + a_log_det)\n",
    "\n",
    "def get_loss_regularization(lam, a):\n",
    "    a_det_sign, a_log_det = np.linalg.slogdet(a)\n",
    "    #assert a_det_sign > 0\n",
    "    return  lam * a_log_det\n",
    "\n",
    "def get_loss(x, lam, a):\n",
    "    loss_by_n = get_loss_by_n(x, a)\n",
    "    return np.sum(loss_by_n) + get_loss_regularization(lam, a)\n",
    "\n",
    "print('Loss at true parameter: {}'.format(get_loss(x, lam, true_a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to minimize the function `loss` using tools like `scipy.optimize.minimize`.  Standard optimization functions take vectors, not matrices, as input, and often require the vector to take valid values in the entire domain.\n",
    "\n",
    "As-written, our loss function takes a positive definite matrix as an input.  We can wrap the loss as a funciton of the free flattened value using the `paragami.FlattenedFunction` class.  The resulting function can be passed directly to `autograd` and `scipy.optimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two losses are the same when evalated on the folded and flat values:\n",
      "\n",
      "Original loss:\t\t2394.604318307261\n",
      "Free-flattened loss: \t2394.604318307261\n",
      "\n",
      "Now, use the flattened function to optimize with autograd.\n",
      "\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2392.424627\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Optimization successful: False\n",
      "Optimal value: 2392.4246267108656\n"
     ]
    }
   ],
   "source": [
    "get_freeflat_loss = paragami.FlattenedFunction(\n",
    "    original_fun=get_loss, patterns=a_pattern, free=True, argnums=2)\n",
    "\n",
    "print('The two losses are the same when evalated on the folded and flat values:\\n')\n",
    "print('Original loss:\\t\\t{}'.format(get_loss(x, lam, true_a)))\n",
    "true_a_freeflat = a_pattern.flatten(true_a, free=True)\n",
    "print('Free-flattened loss: \\t{}'.format(get_freeflat_loss(x, lam, true_a_freeflat)))\n",
    "\n",
    "print('\\nNow, use the flattened function to optimize with autograd.\\n')\n",
    "\n",
    "get_freeflat_loss_grad = autograd.grad(get_freeflat_loss, argnum=2)\n",
    "get_freeflat_loss_hessian = autograd.hessian(get_freeflat_loss, argnum=2)\n",
    "\n",
    "def get_optimum(x, lam):\n",
    "    loss_opt = osp.optimize.minimize(\n",
    "        method='trust-ncg',\n",
    "        x0=np.zeros(a_pattern.flat_length(free=True)),\n",
    "        fun=lambda par: get_freeflat_loss(x, lam, par),\n",
    "        jac=lambda par: get_freeflat_loss_grad(x, lam, par),\n",
    "        hess=lambda par: get_freeflat_loss_hessian(x, lam, par),\n",
    "        options={'gtol': 1e-8, 'disp': True})\n",
    "    return loss_opt\n",
    "\n",
    "loss_opt = get_optimum(x, lam)\n",
    "print('Optimization successful: {}\\nOptimal value: {}'.format(\n",
    "    loss_opt.success, loss_opt.fun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization was in the free flattened space, so to get the optimal value of $A$ we must fold it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True a:\n",
      "[[1.03745401 0.07746864 0.03950388]\n",
      " [0.07746864 2.01560186 0.05110853]\n",
      " [0.03950388 0.05110853 3.0601115 ]]\n",
      "Optimal a:\n",
      "[[ 1.0666994   0.07756251  0.04867278]\n",
      " [ 0.07756251  1.89015495 -0.03092313]\n",
      " [ 0.04867278 -0.03092313  2.93888267]]\n"
     ]
    }
   ],
   "source": [
    "print('True a:\\n{}\\nOptimal a:\\n{}'.format(\n",
    "    true_a,\n",
    "    a_pattern.fold(loss_opt.x, free=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wanted to use the sandwich estimator "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
