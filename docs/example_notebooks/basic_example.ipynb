{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import paragami\n",
    "\n",
    "import autograd\n",
    "from autograd import numpy as np\n",
    "from autograd import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by considering a simple symmetric positive-definite matrix.  In general, we can write:\n",
    "\n",
    "$$\n",
    "A = \\left[\n",
    "\\begin{matrix}\n",
    "a_{11} & a_{12} & a_{13}  \\\\\n",
    "a_{21} & a_{22} & a_{23}  \\\\\n",
    "a_{31} & a_{32} & a_{33}  \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "...although, of course, symmetry and positive-definiteness impose constraints on the entries $a_{ij}$ of $A$ which we will discuss below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are interested in optimizing some function of $A$, say, a sample covariance with a regularization penalty on the log determinant (denoted $\\log |A|$).\n",
    "\n",
    "Let the data be $X = \\left(x_1, ..., x_N\\right)$, where $x_n \\in \\mathbb{R}^3$, and write a loss function as\n",
    "\n",
    "$$\n",
    "\\ell\\left(X, A\\right) = \\sum_{n=1}^N \\left(\\frac{1}{2}x_n^T A x_n - \\lambda \\log |A| \\right).\n",
    "$$\n",
    "\n",
    "Let's simulate some data that is appropriate for this loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n",
      "7460.3271614308505\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "\n",
    "# True value of A\n",
    "true_a = np.eye(3) * np.diag(np.array([1, 2, 3])) + np.random.random((3, 3)) * 0.1\n",
    "true_a = 0.5 * (true_a + true_a.T)\n",
    "\n",
    "# Data\n",
    "x = np.random.multivariate_normal(mean=np.zeros(3), cov=true_a, size=(N, ))\n",
    "print(x.shape)\n",
    "\n",
    "# Regularization constant\n",
    "lam = 1.0\n",
    "\n",
    "def loss(x, lam, a):\n",
    "    a_det_sign, a_log_det = np.linalg.slogdet(a)\n",
    "    assert a_det_sign > 0\n",
    "    return 0.5 * np.einsum('ni,ij,nj', x, a, x) - lam * a_log_det\n",
    "\n",
    "print(loss(x, lam, true_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to minimize the function `loss` using tools like `scipy.optimize.minimize`, but standard optimization functions take vectors, not matrices, as input, and typically require the vector to take valid values in the entire domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first consider how to represent $A$ as a vector, and then as an unconstrained vector.\n",
    "\n",
    "Because $A$ is symmetric, $a_{21} = a_{12}$, $a_{31} = a_{13}$, and $a_{23} = a_{32}$.  This means that to reproduce $A$ we only need to store the bold values:\n",
    "\n",
    "$$\n",
    "A = \\left[\n",
    "\\begin{matrix}\n",
    "\\mathbf{a_{11}} & \\mathbf{a_{12}} & \\mathbf{a_{13}}  \\\\\n",
    "a_{21} & \\mathbf{a_{22}} & \\mathbf{a_{23}}  \\\\\n",
    "a_{31} & a_{32} & \\mathbf{a_{33}}  \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "If we wanted to represent $A$ as an flat array, we could write\n",
    "\n",
    "$$\n",
    "A_{flat} = \\left[\n",
    "\\begin{matrix}\n",
    "\\mathbf{a_{11}} \\\\\n",
    "\\mathbf{a_{12}} \\\\\n",
    "\\mathbf{a_{22}} \\\\\n",
    "\\mathbf{a_{13}} \\\\\n",
    "\\mathbf{a_{23}} \\\\\n",
    "\\mathbf{a_{33}} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "and fully recover $A$ from $A_{flat}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to and from $A$ and $A_{flat}$ can be done with a `paragami.PDMatrixPattern` pattern.  Because we are simply re-stacking the values of $A$ into a vector, not transforming into an unconstrained space, we use `free=False`.\n",
    "\n",
    "Note that `a_flat` contains the elements in precisely the same order as $A_{flat}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [[1.4473274  0.65013957 0.0527136 ]\n",
      " [0.65013957 1.12575218 0.650802  ]\n",
      " [0.0527136  0.650802   1.81343529]]\n",
      "a_flat: [1.4473274  0.65013957 1.12575218 0.0527136  0.650802   1.81343529]\n"
     ]
    }
   ],
   "source": [
    "a = np.eye(3) + np.random.random((3, 3))\n",
    "a = 0.5 * (a + a.T)\n",
    "\n",
    "a_pattern = paragami.PDMatrixPattern(size=3)\n",
    "a_flat = a_pattern.flatten(a, free=False)\n",
    "print('a: {}'.format(a))\n",
    "print('a_flat: {}'.format(a_flat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If we want to modify our function `loss` to take `a_flat` as an input rather than `a`, we can use `paragami.FlattenFunction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_flat = paragami.FlattenedFunction()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
