{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import paragami\n",
    "\n",
    "import autograd\n",
    "from autograd import numpy as np\n",
    "from autograd import scipy as sp\n",
    "\n",
    "# Use the original scipy for functions we don't need to differentiate.\n",
    "import scipy as osp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `paragami` with a positive semi-definite matrix parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by considering a simple symmetric positive semi-definite matrix.  In general, we can write:\n",
    "\n",
    "$$\n",
    "A = \\left[\n",
    "\\begin{matrix}\n",
    "a_{11} & a_{12} & a_{13}  \\\\\n",
    "a_{21} & a_{22} & a_{23}  \\\\\n",
    "a_{31} & a_{32} & a_{33}  \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "...although, of course, symmetry and positive semi-definiteness impose constraints on the entries $a_{ij}$ of $A$ which we will discuss below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening and folding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first consider how to represent $A$ as a vector, and then as an unconstrained vector.\n",
    "\n",
    "Because $A$ is symmetric, $a_{21} = a_{12}$, $a_{31} = a_{13}$, and $a_{23} = a_{32}$.  This means that to reproduce $A$ we only need to store the bold values:\n",
    "\n",
    "$$\n",
    "A = \\left[\n",
    "\\begin{matrix}\n",
    "\\mathbf{a_{11}} & \\mathbf{a_{12}} & \\mathbf{a_{13}}  \\\\\n",
    "a_{21} & \\mathbf{a_{22}} & \\mathbf{a_{23}}  \\\\\n",
    "a_{31} & a_{32} & \\mathbf{a_{33}}  \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "If we wanted to represent $A$ as an flat array, we could write\n",
    "\n",
    "$$\n",
    "A_{flat} = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "a_{flat,1} \\\\\n",
    "a_{flat,2} \\\\\n",
    "a_{flat,3} \\\\\n",
    "a_{flat,4} \\\\\n",
    "a_{flat,5} \\\\\n",
    "a_{flat,6} \\\\\n",
    "\\end{matrix}\n",
    "\\right] =\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\mathbf{a_{11}} \\\\\n",
    "\\mathbf{a_{12}} \\\\\n",
    "\\mathbf{a_{22}} \\\\\n",
    "\\mathbf{a_{13}} \\\\\n",
    "\\mathbf{a_{23}} \\\\\n",
    "\\mathbf{a_{33}} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "and fully recover $A$ from $A_{flat}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to and from $A$ and $A_{flat}$ can be done with the `flatten` method of a `paragami.PSDMatrixPattern` pattern.  \n",
    "\n",
    "For the moment, because we are simply re-stacking the values of $A$ into a vector, not transforming into an unconstrained space, we use the option `free=False`.  We will discuss the `free=True` option shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, a_flat contains the elements of a exactly as shown in the formula above.\n",
      "\n",
      "a:\n",
      "[[1.43029514 0.91026842 0.28169565]\n",
      " [0.91026842 1.96292882 0.67954451]\n",
      " [0.28169565 0.67954451 1.39334116]]\n",
      "\n",
      "a_flat:\n",
      "[1.43029514 0.91026842 1.96292882 0.28169565 0.67954451 1.39334116]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A sample positive semi-definite matrix.\n",
    "a = np.eye(3) + np.random.random((3, 3))\n",
    "a = 0.5 * (a + a.T)\n",
    "\n",
    "# Define a pattern and fold.\n",
    "a_pattern = paragami.PSDMatrixPattern(size=3)\n",
    "a_flat = a_pattern.flatten(a, free=False)\n",
    "\n",
    "print('Now, a_flat contains the elements of a exactly as shown in the formula above.\\n')\n",
    "print('a:\\n{}\\n'.format(a))\n",
    "print('a_flat:\\n{}\\n'.format(a_flat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also convert from $A_{flat}$ back to $A$ by 'folding'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folding the flattened value recovers the original matrix.\n",
      "\n",
      "a:\n",
      "[[1.43029514 0.91026842 0.28169565]\n",
      " [0.91026842 1.96292882 0.67954451]\n",
      " [0.28169565 0.67954451 1.39334116]]\n",
      "\n",
      "a_fold:\n",
      "[1.43029514 0.91026842 1.96292882 0.28169565 0.67954451 1.39334116]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Folding the flattened value recovers the original matrix.\\n')\n",
    "a_fold = a_pattern.fold(a_flat, free=False)\n",
    "print('a:\\n{}\\n'.format(a))\n",
    "print('a_fold:\\n{}\\n'.format(a_flat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free flattening and folding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not every length-six vector represents a valid positive semi-definite matrix.  If the attribute `validate` of a `paragami.PSDMatrixPattern` is true, then folding automatically checks for validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The diagonal of a positive semi-definite matrix must not be less than 0, and folding checks this when validate=True, which it is by default:\n",
      "\n",
      "A bad folded value: [-1  0  0  0  0  0]\n",
      "Folding with a_pattern raised the following ValueError:\n",
      "Diagonal is less than the lower bound 0.0.\n",
      "\n",
      "If validate is false, folding will produce an invalid matrix without an error:\n",
      "\n",
      "Folding a non-pd matrix with validate=False:\n",
      "[[-1.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "\n",
      "However, it will not produce a matrix of the wrong shape even when validate is False:\n",
      "\n",
      "A very bad folded value: [1 0 0].\n",
      "Folding with a_pattern raised the following ValueError:\n",
      "Wrong length for PDMatrix flat value.\n"
     ]
    }
   ],
   "source": [
    "print('The diagonal of a positive semi-definite matrix must not be less',\n",
    "      'than 0, and folding checks this when validate=True, which it is by default:\\n')\n",
    "a_flat_bad = np.array([-1, 0, 0, 0, 0, 0])\n",
    "print('A bad folded value: {}'.format(a_flat_bad))\n",
    "try:\n",
    "    a_fold_bad = a_pattern.fold(a_flat_bad, free=False)\n",
    "except ValueError as err:\n",
    "    print('Folding with a_pattern raised the following ValueError:\\n{}'.format(err))\n",
    "\n",
    "print('\\nIf validate is false, folding will produce an invalid matrix without an error:\\n')\n",
    "a_pattern.validate = False\n",
    "a_fold_bad = a_pattern.fold(a_flat_bad, free=False)\n",
    "print('Folding a non-pd matrix with validate=False:\\n{}'.format(a_fold_bad))\n",
    "\n",
    "print('\\nHowever, it will not produce a matrix of the wrong shape even when validate is False:\\n')\n",
    "a_flat_very_bad = np.array([1, 0, 0])\n",
    "print('A very bad folded value: {}.'.format(a_flat_very_bad))\n",
    "try:\n",
    "    a_fold_very_bad = a_pattern.fold(a_flat_very_bad, free=False)\n",
    "except ValueError as err:\n",
    "    print('Folding with a_pattern raised the following ValueError:\\n{}'.format(err))\n",
    "\n",
    "# Let's set validate back to true.\n",
    "a_pattern.validate = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Sometimes (e.g. when performing optimization) it is convenient to have a flattened value that can legally take any value of the correct length.  For a positive semi-definite matrix, this can be achieved by a Cholesky decomposition.  The details aren't important here, as `paragami` takes care of the transformation behind the scenes with the option `free=True`. \n",
    "\n",
    "We denote the flattened $A$ in the free parameterization as $A_{freeflat}$.  Of course, in general, $A_{freeflat} \\ne A_{flat}$, though each is related to $A$ -- and, hence, to each other -- by a diffemorphism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The free flat value a_freeflat is not immediately recognizable as a.\n",
      "\n",
      "a:\n",
      "[[1.43029514 0.91026842 0.28169565]\n",
      " [0.91026842 1.96292882 0.67954451]\n",
      " [0.28169565 0.67954451 1.39334116]]\n",
      "\n",
      "a_freeflat:\n",
      "[0.17894041 0.76112615 0.16235011 0.23554143 0.42529939 0.07290737]\n",
      "\n",
      "However, it transforms correctly back to a when folded.\n",
      "\n",
      "a_fold:\n",
      "[[1.43029514 0.91026842 0.28169565]\n",
      " [0.91026842 1.96292882 0.67954451]\n",
      " [0.28169565 0.67954451 1.39334116]]\n",
      "\n",
      "Any length-six vector will free fold back to a valid positive semi-definite matrix up to floating point error. Let's draw 100 random vectors, fold them, and check that this is true.\n",
      "\n",
      "Note that you will get an incorrect values or errors if you mix free and non-free folding and flattening! \n",
      "In general, validate cannot be relied upon to catch such errors.\n",
      "\n",
      "a_fold_free_unfree_mix:\n",
      "[[-1.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('The free flat value a_freeflat is not immediately recognizable as a.\\n')\n",
    "a_freeflat = a_pattern.flatten(a, free=True)\n",
    "print('a:\\n{}\\n'.format(a))\n",
    "print('a_freeflat:\\n{}\\n'.format(a_freeflat))\n",
    "\n",
    "print('However, it transforms correctly back to a when folded.\\n')\n",
    "a_freefold = a_pattern.fold(a_freeflat, free=True)\n",
    "print('a_fold:\\n{}\\n'.format(a_freefold))\n",
    "\n",
    "print('Any length-six vector will free fold back to a valid positive',\n",
    "      'semi-definite matrix up to floating point error.',\n",
    "      'Let\\'s draw 100 random vectors, fold them, and check that this is true.\\n')\n",
    "# Draw random free vectors and confirm that they are positive semi definite.\n",
    "def assert_is_pd(mat):\n",
    "    eigvals = np.linalg.eigvals(mat)\n",
    "    assert np.min(eigvals) >= -1e-8\n",
    "for raw in range(100):\n",
    "    a_rand_freeflat = np.random.normal(scale=2, size=(6, ))\n",
    "    a_rand_fold = a_pattern.fold(a_rand_freeflat, free=True)\n",
    "    assert_is_pd(a_rand_fold)\n",
    "\n",
    "\n",
    "print('Note that you will get an incorrect values or errors if you mix',\n",
    "      'free and non-free folding and flattening!',\n",
    "      '\\nIn general, validate cannot be relied upon to catch such errors.\\n')\n",
    "a_pattern.validate = False\n",
    "a_fold_free_unfree_mix = a_pattern.fold(a_freeflat, free=False)\n",
    "print('a_fold_free_unfree_mix:\\n{}\\n'.format(a_fold_bad))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using flattening and folding for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are interested in optimizing some function of $A$, say, a normal model in which the data $x_n \\sim \\mathcal{N}(0, A)$.  Specifically, Let the data be $X = \\left(x_1, ..., x_N\\right)$, where $x_n \\in \\mathbb{R}^3$, and write a loss function as\n",
    "\n",
    "$$\n",
    "\\ell\\left(X, A\\right) =\n",
    "    -\\sum_{n=1}^N \\log P(x_n | A) =\n",
    "    \\frac{1}{2}\\sum_{n=1}^N \\left(x_n^T A^{-1} x_n - \\log|A|\\right) \n",
    "$$\n",
    "\n",
    "Let's simulate some data under this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1000, 3)\n",
      "Loss at true parameter: 2392.751922600241\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_obs = 1000\n",
    "\n",
    "# True value of A\n",
    "true_a = np.eye(3) * np.diag(np.array([1, 2, 3])) + np.random.random((3, 3)) * 0.1\n",
    "true_a = 0.5 * (true_a + true_a.T)\n",
    "\n",
    "# Data\n",
    "def draw_data(num_obs, true_a):\n",
    "    return np.random.multivariate_normal(\n",
    "        mean=np.zeros(3), cov=true_a, size=(num_obs, ))\n",
    "\n",
    "x = draw_data(num_obs, true_a)\n",
    "print('X shape: {}'.format(x.shape))\n",
    "\n",
    "def get_loss(x, a):\n",
    "    num_obs = x.shape[0]\n",
    "    a_inv = np.linalg.inv(a)\n",
    "    a_det_sign, a_log_det = np.linalg.slogdet(a)\n",
    "    assert a_det_sign > 0\n",
    "    return 0.5 * (np.einsum('ni,ij,nj', x, a_inv, x) + num_obs * a_log_det)\n",
    "\n",
    "print('Loss at true parameter: {}'.format(get_loss(x, true_a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to minimize the function `loss` using tools like `scipy.optimize.minimize`.  Standard optimization functions take vectors, not matrices, as input, and often require the vector to take valid values in the entire domain.\n",
    "\n",
    "As-written, our loss function takes a positive definite matrix as an input.  We can wrap the loss as a funciton of the free flattened value using the `paragami.FlattenedFunction` class.  The resulting function can be passed directly to `autograd` and `scipy.optimize`, and we can estimate\n",
    "\n",
    "$$\n",
    "\\hat{A}_{freeflat} := \\mathrm{argmin}_{A_{freeflat}} \\ell_{freeflat}(X, A_{freeflat})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two losses are the same when evalated on the folded and flat values:\n",
      "\n",
      "Original loss:\t\t2392.751922600241\n",
      "Free-flattened loss: \t2392.751922600241\n",
      "\n",
      "Now, use the flattened function to optimize with autograd.\n",
      "\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2390.646332\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Optimization successful: False\n",
      "Optimal value: 2390.6463320487683\n"
     ]
    }
   ],
   "source": [
    "# The arguments mean we're flatting the function get_loss, using\n",
    "# the pattern a_pattern, with free parameterization, and the paramater\n",
    "# is the second one (argnums uses 0-indexing like autograd).\n",
    "get_freeflat_loss = paragami.FlattenedFunction(\n",
    "    original_fun=get_loss, patterns=a_pattern, free=True, argnums=1)\n",
    "\n",
    "print('The two losses are the same when evalated on the folded and flat values:\\n')\n",
    "print('Original loss:\\t\\t{}'.format(get_loss(x, true_a)))\n",
    "true_a_freeflat = a_pattern.flatten(true_a, free=True)\n",
    "print('Free-flattened loss: \\t{}'.format(\n",
    "    get_freeflat_loss(x, true_a_freeflat)))\n",
    "\n",
    "print('\\nNow, use the flattened function to optimize with autograd.\\n')\n",
    "\n",
    "get_freeflat_loss_grad = autograd.grad(get_freeflat_loss, argnum=1)\n",
    "get_freeflat_loss_hessian = autograd.hessian(get_freeflat_loss, argnum=1)\n",
    "\n",
    "def get_optimum(x):\n",
    "    loss_opt = osp.optimize.minimize(\n",
    "        method='trust-ncg',\n",
    "        x0=np.zeros(a_pattern.flat_length(free=True)),\n",
    "        fun=lambda par: get_freeflat_loss(x, par),\n",
    "        jac=lambda par: get_freeflat_loss_grad(x, par),\n",
    "        hess=lambda par: get_freeflat_loss_hessian(x, par),\n",
    "        options={'gtol': 1e-8, 'disp': True})\n",
    "    return loss_opt\n",
    "\n",
    "loss_opt = get_optimum(x)\n",
    "print('Optimization successful: {}\\nOptimal value: {}'.format(\n",
    "    loss_opt.success, loss_opt.fun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization was in the free flattened space, so to get the optimal value of $A$ we must fold it.   We can see that the optimial value is close to the true value of $A$, though it differs due to randomness in $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True a:\n",
      "[[1.03745401 0.07746864 0.03950388]\n",
      " [0.07746864 2.01560186 0.05110853]\n",
      " [0.03950388 0.05110853 3.0601115 ]]\n",
      "\n",
      "Optimal a:\n",
      "[[ 1.0688328   0.07771763  0.04877013]\n",
      " [ 0.07771763  1.89393526 -0.03098498]\n",
      " [ 0.04877013 -0.03098498  2.94476044]]\n"
     ]
    }
   ],
   "source": [
    "optimal_freeflat_a = loss_opt.x\n",
    "optimal_a = a_pattern.fold(optimal_freeflat_a, free=True)\n",
    "print('True a:\\n{}\\n\\nOptimal a:\\n{}'.format(true_a, optimal_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using flattening and folding for uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wanted to use the Hessian of the objective (the observed Fisher information) to estimate a frequentist confidence region for $A$.  Typically a covariance is of a vector, so we can write what we want in terms of $A_{flat}$:\n",
    "\n",
    "$$\n",
    "\\mathrm{Cov}(A_{flat}) \\approx \\left(\\frac{\\partial^2 \\ell}{\\partial A_{flat} \\partial A_{flat}^T}\\right)^{-1}.\n",
    "$$\n",
    "\n",
    "The covariance between two elements of $A_{flat}$ corresponds to that between two elements of $A$.  For example, using the notation given above,\n",
    "\n",
    "$$\n",
    "\\mathrm{Cov}(a_{flat,1}, a_{flat,2}) = \\mathrm{Cov}(a_{11}, a_{12}) = \\mathrm{Cov}(a_{11}, a_{21})\\\\\n",
    "\\mathrm{Var}(a_{flat,4}) = \\mathrm{Var}(a_{13}) = \\mathrm{Var}(a_{31}),\n",
    "$$\n",
    "etc.\n",
    "\n",
    "Let us see how we can use flattening and folding to make estimating these covariances more convenient.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we note that we cannot use the Hessian of the objective with respect to the free flattened value of $A$.  That would give us a confidence region in the unconstrained paremeterization, whereas we want a confidence region for $A$ itself.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 \\ell}{\\partial A_{freeflat} \\partial A_{freeflat}^T} \\ne\n",
    "\\frac{\\partial^2 \\ell}{\\partial A_{free} \\partial A_{free}^T}.\n",
    "$$\n",
    "\n",
    "However, because the dimension of $A_{flat}$ is the same as that of $A_{freeflat}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: the gradient is also zero with respect to the non-flattened parameter at the optimum.\n",
      "Norm of flattened gradient: 4.564048764418554e-08\n"
     ]
    }
   ],
   "source": [
    "# Get the loss as a function of the flattened parameter.\n",
    "get_flat_loss = paragami.FlattenedFunction(\n",
    "    original_fun=get_loss, patterns=a_pattern, free=False, argnums=1)\n",
    "get_flat_loss_grad = autograd.grad(get_flat_loss, argnum=1)\n",
    "get_flat_loss_hessian = autograd.hessian(get_flat_loss, argnum=1)\n",
    "\n",
    "# Make sure that the optimal value for the free flattened parameter is\n",
    "# also optimal for the flattened parameter.  In general this may not\n",
    "# be true if the unconstraining parameterization changes the dimensionality,\n",
    "# (as with a simplex for example).\n",
    "optimal_flat_a = a_pattern.flatten(optimal_a, free=False)\n",
    "print('Sanity check: the gradient is also zero with respect',\n",
    "      'to the non-flattened parameter at the optimum.')\n",
    "print('Norm of flattened gradient: {}'.format(\n",
    "      np.linalg.norm(get_flat_loss_grad(x, optimal_flat_a))))\n",
    "\n",
    "# Estimate the covariance of the flattened value using the Hessian at the optimum.\n",
    "a_flattened_cov = np.linalg.inv(get_flat_loss_hessian(x, optimal_flat_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting covariance matrix is 6x6, which is the shape of the flattened vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the covariance matrix: (6, 6)\n",
      "Length of the flattened vector: 6\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the covariance matrix: {}'.format(a_flattened_cov.shape))\n",
    "print('Length of the flattened vector: {}'.format(a_pattern.flat_length(free=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shape is inconvenient because it's not obvious visually which entry of the flattened vector corresponds to which element of $A$.  Again, we can use folding to put the estimated marginal standard deviations in a readable shape.\n",
    "\n",
    "Because the result is not a valid covariance matrix, and we are just using the pattern for its shape, we set `verify` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The marginal standard deviations of the elements of A:\n",
      "[[0.04779966 0.0450593  0.05612339]\n",
      " [0.0450593  0.08469936 0.07468698]\n",
      " [0.05612339 0.07468698 0.13169369]]\n"
     ]
    }
   ],
   "source": [
    "a_pattern.verify = False\n",
    "a_sd = a_pattern.fold(np.sqrt(-np.diag(a_flattened_cov)), free=False)\n",
    "print('The marginal standard deviations of the elements of A:\\n{}'.format(a_sd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
