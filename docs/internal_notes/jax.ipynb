{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import grad, hessian, jit, vmap\n",
    "\n",
    "def logprob_fun(mu, x):\n",
    "    return np.sum(0.5 * (mu - x)**2)\n",
    "\n",
    "grad_fun = jit(grad(logprob_fun))\n",
    "hess_fun = jit(hessian(logprob_fun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragami debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paragami\n",
    "import copy\n",
    "import unittest\n",
    "from numpy.testing import assert_array_almost_equal\n",
    "import scipy as sp\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_pattern(testcase, pattern, valid_value,\n",
    "                  check_equal=assert_array_almost_equal,\n",
    "                  jacobian_ad_test=True):\n",
    "\n",
    "    print('Testing pattern {}'.format(pattern))\n",
    "\n",
    "    # Execute required methods.\n",
    "    empty_val = pattern.empty(valid=True)\n",
    "    pattern.flatten(empty_val, free=False)\n",
    "    empty_val = pattern.empty(valid=False)\n",
    "\n",
    "    random_val = pattern.random()\n",
    "    pattern.flatten(random_val, free=False)\n",
    "\n",
    "    str(pattern)\n",
    "\n",
    "    pattern.empty_bool(True)\n",
    "\n",
    "    # Make sure to test != using a custom test.\n",
    "    testcase.assertTrue(pattern == pattern)\n",
    "\n",
    "    ###############################\n",
    "    # Test folding and unfolding.\n",
    "    for free in [True, False, None]:\n",
    "        for free_default in [True, False, None]:\n",
    "            pattern.free_default = free_default\n",
    "            if (free_default is None) and (free is None):\n",
    "                with testcase.assertRaises(ValueError):\n",
    "                    flat_val = pattern.flatten(valid_value, free=free)\n",
    "                with testcase.assertRaises(ValueError):\n",
    "                    folded_val = pattern.fold(flat_val, free=free)\n",
    "            else:\n",
    "                flat_val = pattern.flatten(valid_value, free=free)\n",
    "                testcase.assertEqual(len(flat_val), pattern.flat_length(free))\n",
    "                folded_val = pattern.fold(flat_val, free=free)\n",
    "                check_equal(valid_value, folded_val)\n",
    "                if hasattr(valid_value, 'shape'):\n",
    "                    testcase.assertEqual(valid_value.shape, folded_val.shape)\n",
    "\n",
    "    ####################################\n",
    "    # Test conversion to and from JSON.\n",
    "    pattern_dict = pattern.as_dict()\n",
    "    json_typename = pattern.json_typename()\n",
    "    json_string = pattern.to_json()\n",
    "    json_dict = json.loads(json_string)\n",
    "    testcase.assertTrue('pattern' in json_dict.keys())\n",
    "    testcase.assertTrue(json_dict['pattern'] == json_typename)\n",
    "    new_pattern = paragami.get_pattern_from_json(json_string)\n",
    "    testcase.assertTrue(new_pattern == pattern)\n",
    "\n",
    "    # Test that you cannot covert from a different patter.\n",
    "    bad_test_pattern = BadTestPattern()\n",
    "    bad_json_string = bad_test_pattern.to_json()\n",
    "    testcase.assertFalse(pattern == bad_test_pattern)\n",
    "    testcase.assertRaises(\n",
    "        ValueError,\n",
    "        lambda: pattern.__class__.from_json(bad_json_string))\n",
    "\n",
    "    ############################################\n",
    "    # Test the freeing and unfreeing Jacobians.\n",
    "    def freeing_transform(flat_val):\n",
    "        return pattern.flatten(\n",
    "            pattern.fold(flat_val, free=False), free=True)\n",
    "\n",
    "    def unfreeing_transform(free_flat_val):\n",
    "        return pattern.flatten(\n",
    "            pattern.fold(free_flat_val, free=True), free=False)\n",
    "\n",
    "    ad_freeing_jacobian = jax.jacobian(freeing_transform)\n",
    "    ad_unfreeing_jacobian = jax.jacobian(unfreeing_transform)\n",
    "\n",
    "    for sparse in [True, False]:\n",
    "        flat_val = pattern.flatten(valid_value, free=False)\n",
    "        freeflat_val = pattern.flatten(valid_value, free=True)\n",
    "        freeing_jac = pattern.freeing_jacobian(valid_value, sparse)\n",
    "        unfreeing_jac = pattern.unfreeing_jacobian(valid_value, sparse)\n",
    "        free_len = pattern.flat_length(free=False)\n",
    "        flatfree_len = pattern.flat_length(free=True)\n",
    "\n",
    "        # Check the shapes.\n",
    "        testcase.assertTrue(freeing_jac.shape == (flatfree_len, free_len))\n",
    "        testcase.assertTrue(unfreeing_jac.shape == (free_len, flatfree_len))\n",
    "\n",
    "        # Check the values of the Jacobians.\n",
    "        if sparse:\n",
    "            # The Jacobians should be inverses of one another and full rank\n",
    "            # in the free flat space.\n",
    "            assert_array_almost_equal(\n",
    "                np.eye(flatfree_len),\n",
    "                np.array((freeing_jac @ unfreeing_jac).todense()))\n",
    "            if jacobian_ad_test:\n",
    "                assert_array_almost_equal(\n",
    "                    ad_freeing_jacobian(flat_val),\n",
    "                    np.array(freeing_jac.todense()))\n",
    "                assert_array_almost_equal(\n",
    "                    ad_unfreeing_jacobian(freeflat_val),\n",
    "                    np.array(unfreeing_jac.todense()))\n",
    "        else:\n",
    "            # The Jacobians should be inverses of one another and full rank\n",
    "            # in the free flat space.\n",
    "            assert_array_almost_equal(\n",
    "                np.eye(flatfree_len), freeing_jac @ unfreeing_jac)\n",
    "            if jacobian_ad_test:\n",
    "                assert_array_almost_equal(\n",
    "                    ad_freeing_jacobian(flat_val), freeing_jac)\n",
    "                assert_array_almost_equal(\n",
    "                    ad_unfreeing_jacobian(freeflat_val), unfreeing_jac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/paragami/venv/lib/python3.6/site-packages/jax/numpy/lax_numpy.py:1621: FutureWarning: jax.numpy reductions won't accept lists and tuples in future versions, only scalars and ndarrays\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/home/rgiordan/Documents/git_repos/paragami/venv/lib/python3.6/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30133385 0.38058881 0.31807734]\n",
      " [0.2895409  0.34676119 0.36369791]]\n",
      "[ 0.00000000e+00 -1.11022302e-16]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.30133385, 0.38058881, 0.31807734, 0.2895409 , 0.34676119,\n",
       "             0.36369791], dtype=float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = paragami.SimplexArrayPattern(3, (2, ))\n",
    "\n",
    "empty_val = pattern.empty(valid=True)\n",
    "pattern.flatten(empty_val, free=False)\n",
    "\n",
    "random_val = pattern.random()\n",
    "print(random_val)\n",
    "print(np.sum(random_val, axis=-1) - 1)\n",
    "pattern.flatten(random_val, free=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a871fdc9ebee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_shape_and_size(simplex_size, array_shape):\n",
    "    class DummyTest(unittest.TestCase):\n",
    "        pass\n",
    "\n",
    "    shape = array_shape + (simplex_size, )\n",
    "    valid_value = np.random.random(shape) + 0.1\n",
    "    valid_value = \\\n",
    "        valid_value / np.sum(valid_value, axis=-1, keepdims=True)\n",
    "\n",
    "    pattern = paragami.SimplexArrayPattern(simplex_size, array_shape)\n",
    "    _test_pattern(DummyTest(), pattern, valid_value)\n",
    "\n",
    "test_shape_and_size(4, (2, 3))\n",
    "# test_shape_and_size(2, (2, 3))\n",
    "# test_shape_and_size(2, (2, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Jax stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = onp.random.random(100)\n",
    "mu = onp.random.random(100)\n",
    "\n",
    "# I would have expected this to get rid of the annoying warning but it does not.\n",
    "cpu_device = jax.devices('cpu')[0]\n",
    "jax.device_put(x, cpu_device);\n",
    "jax.device_put(mu, cpu_device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = grad_fun(mu, x)\n",
    "h = hess_fun(mu, x)\n",
    "print(np.max(np.abs(g - (mu - x))))\n",
    "print(np.max(np.abs(h - np.eye(100))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import custom_jvp\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# f :: a -> b\n",
    "@custom_jvp\n",
    "def f(x):\n",
    "    return jnp.sin(x)\n",
    "\n",
    "# f_jvp :: (a, T a) -> (b, T b)\n",
    "def f_jvp(primals, tangents):\n",
    "    x, = primals\n",
    "    t, = tangents\n",
    "    return f(x), jnp.cos(x) * t\n",
    "\n",
    "f.defjvp(f_jvp)\n",
    "\n",
    "print(type(f(0.5)))\n",
    "\n",
    "print('Use jax')\n",
    "foo = jax.numpy.asarray(f(0.5) + 3)\n",
    "print(foo, type(foo))\n",
    "print(isinstance(foo, onp.ndarray))\n",
    "print(isinstance(foo, jax.numpy.ndarray))\n",
    "\n",
    "print('Use numpy')\n",
    "foo = onp.asarray(f(0.5) + 3)\n",
    "print(isinstance(foo, onp.ndarray))\n",
    "print(isinstance(foo, jax.numpy.ndarray))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@custom_jvp\n",
    "def f(x, y):\n",
    "    return jnp.sin(x) * y\n",
    "\n",
    "f.defjvps(lambda x_dot, primal_out, x, y: jnp.cos(x) * x_dot * y,\n",
    "          lambda y_dot, primal_out, x, y: jnp.sin(x) * y_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = np.array([1, 2, 3])\n",
    "print(jax.ops.index_update(foo, [1, 2], [10, 20]))\n",
    "\n",
    "foo = np.array([[1, 2], [3, 4]])\n",
    "print(jax.ops.index_update(foo, [1, 2], [10, 20]))\n",
    "\n",
    "inds = np.triu_indices(2)\n",
    "print(inds)\n",
    "print(jax.ops.index_update(foo, inds, [10, 20, 30]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jax.sp.logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = np.arange(0, 3, dtype=np.float32) + 1\n",
    "np.diag(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _exp_matrix_diagonal(mat):\n",
    "    assert mat.shape[0] == mat.shape[1]\n",
    "    dim = mat.shape[0]\n",
    "    diag_inds = (np.arange(dim), np.arange(dim))\n",
    "    exp_diags = np.exp(np.diag(mat))\n",
    "    return(jax.ops.index_update(mat, diag_inds, exp_diags))\n",
    "\n",
    "def _log_matrix_diagonal(mat):\n",
    "    assert mat.shape[0] == mat.shape[1]\n",
    "    dim = mat.shape[0]\n",
    "    diag_inds = (np.arange(dim), np.arange(dim))\n",
    "    log_diags = np.log(np.diag(mat))\n",
    "    return(jax.ops.index_update(mat, diag_inds, log_diags))\n",
    "\n",
    "mat = onp.random.random((3, 3))\n",
    "print(mat)\n",
    "print(_exp_matrix_diagonal(mat))\n",
    "print(jax.jacobian(_exp_matrix_diagonal)(mat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.triu_indices(5)\n",
    "\n",
    "def pack_vec(vec, dim):\n",
    "    assert len(vec) == dim * (dim + 1) / 2\n",
    "    mat = np.zeros((dim, dim))\n",
    "    inds = np.tril_indices(dim)\n",
    "    return(jax.ops.index_update(mat, inds, vec))\n",
    "\n",
    "vec = np.arange(0, 6, dtype=np.float32) + 1\n",
    "print(vec.dtype)\n",
    "print(pack_vec(vec, 3))\n",
    "\n",
    "print('Raw:')\n",
    "print(jax.jacobian(pack_vec)(vec, 3))\n",
    "\n",
    "print('JIT:')\n",
    "jac_fun = jit(jax.jacobian(pack_vec), static_argnums=1)\n",
    "print(jac_fun(vec, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fails\n",
    "\n",
    "# @custom_jvp\n",
    "# def replace_ind(x, v, i):\n",
    "#     x[i] = v\n",
    "#     return x\n",
    "\n",
    "# replace_ind.defjvps(\n",
    "#     lambda x_dot, ans, x, v, i: replace_ind(x_dot, 0.0, i),\n",
    "#     lambda v_dot, ans, x, v, i: replace_ind(ans, v_dot, i),\n",
    "#     None)\n",
    "\n",
    "# x = np.array([1.0, 2.0, 3.5])\n",
    "# replace_ind(x, 10.0, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paragami_sandbox",
   "language": "python",
   "name": "paragami_sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
