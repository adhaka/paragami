{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/paragami/venv/lib/python3.6/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "from paragami.base_patterns import Pattern\n",
    "from paragami.pattern_containers import register_pattern_json\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "\n",
    "import math\n",
    "\n",
    "import time\n",
    "import timeit\n",
    "#from jax import custom_jvp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def assert_equal(x, y, tol=1e-12):\n",
    "    assert(onp.max(onp.abs(x - y)) < tol)\n",
    "    \n",
    "def time_jit(f):\n",
    "    tic = time.time()\n",
    "    f()\n",
    "    print('1st time: ', time.time() - tic)\n",
    "\n",
    "    tic = time.time()\n",
    "    f()\n",
    "    print('2nd time: ', time.time() - tic)\n",
    "\n",
    "    \n",
    "def mark_tic(tic, op):\n",
    "    print(f'{op}:\\t{time.time() - tic}')\n",
    "    return time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paragami\n",
    "\n",
    "k_approx = 30\n",
    "dim = 4\n",
    "\n",
    "base_pattern = paragami.PSDSymmetricMatrixPattern(size=dim)\n",
    "covar_array_pattern = \\\n",
    "        paragami.PatternArray(array_shape = (k_approx, ), \\\n",
    "                    base_pattern = base_pattern)\n",
    "\n",
    "covar_array = covar_array_pattern.random()\n",
    "\n",
    "covar_array_flattened = covar_array_pattern.flatten(covar_array, free = False)\n",
    "covar_array_flattened_free = covar_array_pattern.flatten(covar_array, free = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(covar_array): \n",
    "    return (covar_array**2).sum()\n",
    "\n",
    "# flattened function\n",
    "fun_flattened = paragami.FlattenFunctionInput(original_fun=fun, \n",
    "                                patterns = covar_array_pattern,\n",
    "                                free = False,\n",
    "                                argnums = 0) \n",
    "\n",
    "# flattened and freed function\n",
    "fun_flattened_free = paragami.FlattenFunctionInput(original_fun=fun, \n",
    "                                patterns = covar_array_pattern,\n",
    "                                free = True,\n",
    "                               argnums = 0) \n",
    "\n",
    "assert_equal(\n",
    "    fun_flattened(covar_array_flattened),\n",
    "    fun_flattened_free(covar_array_flattened_free))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st time:  0.8329370021820068\n",
      "2nd time:  0.0001614093780517578\n",
      "1st time:  14.864873886108398\n",
      "2nd time:  0.0004405975341796875\n"
     ]
    }
   ],
   "source": [
    "grad_fun_flattened = jax.jit(jax.grad(fun_flattened))\n",
    "grad_fun_flattened_free = jax.jit(jax.grad(fun_flattened_free))\n",
    "\n",
    "time_jit(lambda: grad_fun_flattened(covar_array_flattened))\n",
    "time_jit(lambda: grad_fun_flattened_free(covar_array_flattened_free))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "print(jax.lax.map(lambda x: x + 1, np.arange(0, 10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "__array_shape = (2, 3)\n",
    "__array_ranges = [range(0, t) for t in __array_shape]\n",
    "__array_ranges\n",
    "\n",
    "\n",
    "empty_pattern = base_pattern.empty(valid=True)\n",
    "__shape = tuple(__array_shape) + empty_pattern.shape\n",
    "\n",
    "repeated_array = np.array(\n",
    "    [empty_pattern\n",
    "     for item in itertools.product(*__array_ranges)])\n",
    "empty_orig = np.reshape(repeated_array, __shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works but inefficient, you don't want to create the entries array.\n",
    "entries = np.array([ i for i in itertools.product(*__array_ranges) ])\n",
    "repeated_array = jax.lax.map(lambda x: empty_pattern, entries)\n",
    "empty_jax = np.reshape(repeated_array, __shape)\n",
    "assert_equal(empty_jax, empty_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSD patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a871fdc9ebee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _sym_index(k1, k2):\n",
    "    \"\"\"\n",
    "    Get the index of an entry in a folded symmetric array.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    k1, k2: int\n",
    "        0-based indices into a symmetric matrix.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    int\n",
    "        Return the linear index of the (k1, k2) element of a symmetric\n",
    "        matrix where the triangular part has been stacked into a vector.\n",
    "    \"\"\"\n",
    "    def ld_ind(k1, k2):\n",
    "        return int(k2 + k1 * (k1 + 1) / 2)\n",
    "\n",
    "    if k2 <= k1:\n",
    "        return ld_ind(k1, k2)\n",
    "    else:\n",
    "        return ld_ind(k2, k1)\n",
    "\n",
    "\n",
    "def _vectorize_ld_matrix(mat):\n",
    "    \"\"\"\n",
    "    Linearize the lower diagonal of a square matrix.\n",
    "    \"\"\"\n",
    "    nrow, ncol = np.shape(mat)\n",
    "    if nrow != ncol:\n",
    "        raise ValueError('mat must be square')\n",
    "    return mat[np.tril_indices(nrow)]\n",
    "\n",
    "\n",
    "def _unvectorize_ld_matrix(vec):\n",
    "    \"\"\"\n",
    "    Invert the mapping of `_vectorize_ld_matrix`.\n",
    "    \"\"\"\n",
    "    mat_size = int(0.5 * (math.sqrt(1 + 8 * vec.size) - 1))\n",
    "    if mat_size * (mat_size + 1) / 2 != vec.size:\n",
    "        raise ValueError('Vector is an impossible size')\n",
    "\n",
    "    mat = np.zeros((mat_size, mat_size))\n",
    "    inds = np.tril_indices(mat_size)\n",
    "    return(jax.ops.index_update(mat, inds, vec))\n",
    "\n",
    "\n",
    "def _exp_matrix_diagonal(mat):\n",
    "    assert mat.shape[0] == mat.shape[1]\n",
    "    dim = mat.shape[0]\n",
    "    diag_inds = (np.arange(dim), np.arange(dim))\n",
    "    exp_diags = np.exp(np.diag(mat))\n",
    "    return(jax.ops.index_update(mat, diag_inds, exp_diags))\n",
    "\n",
    "\n",
    "def _log_matrix_diagonal(mat):\n",
    "    assert mat.shape[0] == mat.shape[1]\n",
    "    dim = mat.shape[0]\n",
    "    diag_inds = (np.arange(dim), np.arange(dim))\n",
    "    log_diags = np.log(np.diag(mat))\n",
    "    return(jax.ops.index_update(mat, diag_inds, log_diags))\n",
    "\n",
    "\n",
    "def _pack_posdef_matrix(mat, diag_lb=0.0):\n",
    "    k = mat.shape[0]\n",
    "    # mat_lb = mat - np.make_diagonal(\n",
    "    #     np.full(k, diag_lb), offset=0, axis1=-1, axis2=-2)\n",
    "    mat_lb = mat - np.diag(np.full(k, diag_lb))\n",
    "    return _vectorize_ld_matrix(\n",
    "        _log_matrix_diagonal(np.linalg.cholesky(mat_lb)))\n",
    "\n",
    "\n",
    "def _unpack_posdef_matrix(free_vec, diag_lb=0.0):\n",
    "    mat_raw = _unvectorize_ld_matrix(free_vec)\n",
    "    #return mat_raw # 0.20\n",
    "\n",
    "    mat_chol = _exp_matrix_diagonal(mat_raw)\n",
    "    #return mat_chol # 0.60\n",
    "    #mat_chol = mat_raw\n",
    "\n",
    "    # Doesn't seem to matter much what you do\n",
    "    #mat = np.einsum('ik,jk->ij', mat_chol, mat_chol)\n",
    "    mat = np.matmul(mat_chol, np.transpose(mat_chol))\n",
    "    #return mat # 1.3\n",
    "\n",
    "    dim = mat.shape[0]\n",
    "    diag_inds = (np.arange(dim), np.arange(dim))\n",
    "    new_mat = jax.ops.index_update(mat, diag_inds, np.diag(mat) + diag_lb)\n",
    "    return new_mat # 1.1 ?!\n",
    "\n",
    "    \n",
    "# Convert a vector containing the lower diagonal portion of a symmetric\n",
    "# matrix into the full symmetric matrix.\n",
    "#\n",
    "# This is not currently used but could be useful for a symmetric matrix type.\n",
    "def _unvectorize_symmetric_matrix(vec_val):\n",
    "    ld_mat = _unvectorize_ld_matrix(vec_val)\n",
    "    mat_val = ld_mat + ld_mat.transpose()\n",
    "    # We have double counted the diagonal.  For some reason the autograd\n",
    "    # diagonal functions require axis1=-1 and axis2=-2\n",
    "    # mat_val = mat_val - \\\n",
    "    #     np.make_diagonal(np.diagonal(ld_mat, axis1=-1, axis2=-2),\n",
    "    #                      axis1=-1, axis2=-2)\n",
    "    mat_val = mat_val - np.diag(np.diagonal(ld_mat))\n",
    "\n",
    "    return mat_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50\n",
    "#mat = np.eye(dim) * dim + np.full((dim, dim), 0.1)\n",
    "foo = onp.random.random((dim, dim))\n",
    "mat = np.eye(dim) * dim + foo + foo.T\n",
    "mat = np.array(mat)\n",
    "vec = _pack_posdef_matrix(mat)\n",
    "assert_equal(_unpack_posdef_matrix(vec), mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st time:  1.1781713962554932\n",
      "2nd time:  0.0005805492401123047\n",
      "subsequent:\t0.0006048679351806641\n",
      "subsequent but new par:\t1.2682888507843018\n",
      "subsequent 0.0:\t0.00041222572326660156\n",
      "subsequent 0.1:\t0.00035071372985839844\n"
     ]
    }
   ],
   "source": [
    "pack_grad = jax.jit(jax.jacobian(_pack_posdef_matrix))\n",
    "\n",
    "unpack_grad = jax.jit(jax.jacobian(\n",
    "    lambda x, diag_lb: _unpack_posdef_matrix(x, diag_lb=diag_lb)),\n",
    "                      static_argnums=1)\n",
    "time_jit(lambda: unpack_grad(vec, 0.0))\n",
    "\n",
    "tic = time.time()\n",
    "unpack_grad(vec, 0.0)\n",
    "tic = mark_tic(tic, 'subsequent')\n",
    "unpack_grad(vec, 0.1)\n",
    "tic = mark_tic(tic, 'subsequent but new par')\n",
    "unpack_grad(vec, 0.1)\n",
    "tic = mark_tic(tic, 'subsequent 0.0')\n",
    "unpack_grad(vec, 0.0);\n",
    "tic = mark_tic(tic, 'subsequent 0.1')\n",
    "unpack_grad(vec, 0.1);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp\n",
      "1st time:  0.8487815856933594\n",
      "2nd time:  0.0005159378051757812\n",
      "1st time:  0.0006289482116699219\n",
      "2nd time:  0.0005216598510742188\n",
      "1st time:  0.0005910396575927734\n",
      "2nd time:  0.0005042552947998047\n",
      "Pack\n",
      "1st time:  0.1282823085784912\n",
      "2nd time:  0.00017905235290527344\n",
      "Unpack\n",
      "1st time:  0.25592041015625\n",
      "2nd time:  0.00032973289489746094\n"
     ]
    }
   ],
   "source": [
    "exp_grad = jax.jit(jax.jacobian(_exp_matrix_diagonal))\n",
    "\n",
    "print('Exp')\n",
    "time_jit(lambda: exp_grad(mat))\n",
    "\n",
    "time_jit(lambda: np.matmul(mat, mat.T))\n",
    "time_jit(lambda: np.matmul(mat, np.transpose(mat)))\n",
    "\n",
    "\n",
    "pack_grad = jax.jit(jax.jacobian(_vectorize_ld_matrix))\n",
    "unpack_grad = jax.jit(jax.jacobian(_unvectorize_ld_matrix))\n",
    "\n",
    "print('Pack')\n",
    "time_jit(lambda: pack_grad(mat))\n",
    "\n",
    "print('Unpack')\n",
    "time_jit(lambda: unpack_grad(vec))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paragami_sandbox",
   "language": "python",
   "name": "paragami_sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
