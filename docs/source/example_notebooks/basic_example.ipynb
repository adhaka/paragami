{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folding and Unfolding Example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import paragami\n",
    "\n",
    "import autograd\n",
    "from autograd import numpy as np\n",
    "from autograd import scipy as sp\n",
    "\n",
    "# Use the original scipy for functions we don't need to differentiate.\n",
    "import scipy as osp\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will consider flattening and folding a simple symmetric positive semi-definite matrix:\n",
    "\n",
    "$$\n",
    "A = \\left[\n",
    "\\begin{matrix}\n",
    "a_{11} & a_{12} & a_{13}  \\\\\n",
    "a_{21} & a_{22} & a_{23}  \\\\\n",
    "a_{31} & a_{32} & a_{33}  \\\\\n",
    "\\end{matrix}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "Of course, symmetry and positive semi-definiteness impose constraints on the entries $a_{ij}$ of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening and folding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first consider how to represent $A$ as a vector, which we call simply *flattening*, and then as an unconstrained vector, which we call *free flattening*.\n",
    "\n",
    "When a parameter is flattened, it is simply re-shaped as a vector.  Every number that was in the original parameter will occur exactly once in the flattened shape.  (In the present case of a matrix, this is exactly the same as ``np.flatten``.)\n",
    "\n",
    "$$\n",
    "A = \\left[\n",
    "\\begin{matrix}\n",
    "a_{11} & a_{12} & a_{13}  \\\\\n",
    "a_{21} & a_{22} & a_{23}  \\\\\n",
    "a_{31} & a_{32} & a_{33}  \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\xrightarrow{flatten}\n",
    "A_{flat} = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "a_{flat,1} \\\\\n",
    "a_{flat,2} \\\\\n",
    "a_{flat,3} \\\\\n",
    "a_{flat,4} \\\\\n",
    "a_{flat,5} \\\\\n",
    "a_{flat,6} \\\\\n",
    "a_{flat,7} \\\\\n",
    "a_{flat,8} \\\\\n",
    "a_{flat,9} \\\\\n",
    "\\end{matrix}\\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "a_{11} \\\\\n",
    "a_{12} \\\\\n",
    "a_{13} \\\\\n",
    "a_{21} \\\\\n",
    "a_{22} \\\\\n",
    "a_{23} \\\\\n",
    "a_{31} \\\\\n",
    "a_{32} \\\\\n",
    "a_{33} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to and from $A$ and $A_{flat}$ can be done with the `flatten` method of a `paragami.PSDSymmetricMatrixPattern` pattern.  \n",
    "\n",
    "For the moment, because we are flattening, not free flattening, we use the option `free=False`.  We will discuss the `free=True` option shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, a_flat contains the elements of a exactly as shown in the formula above.\n",
      "\n",
      "a:\n",
      "[[1.4328878  0.44698645 0.41960118]\n",
      " [0.44698645 1.63689837 0.24465332]\n",
      " [0.41960118 0.24465332 1.67981214]]\n",
      "\n",
      "a_flat:\n",
      "[1.4328878  0.44698645 0.41960118 0.44698645 1.63689837 0.24465332\n",
      " 0.41960118 0.24465332 1.67981214]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A sample positive semi-definite matrix.\n",
    "a = np.eye(3) + np.random.random((3, 3))\n",
    "a = 0.5 * (a + a.T)\n",
    "\n",
    "# Define a pattern and fold.\n",
    "a_pattern = paragami.PSDSymmetricMatrixPattern(size=3)\n",
    "a_flat = a_pattern.flatten(a, free=False)\n",
    "\n",
    "print('Now, a_flat contains the elements of a exactly as shown in the formula above.\\n')\n",
    "print('a:\\n{}\\n'.format(a))\n",
    "print('a_flat:\\n{}\\n'.format(a_flat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also convert from $A_{flat}$ back to $A$ by 'folding'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folding the flattened value recovers the original matrix.\n",
      "\n",
      "a:\n",
      "[[1.4328878  0.44698645 0.41960118]\n",
      " [0.44698645 1.63689837 0.24465332]\n",
      " [0.41960118 0.24465332 1.67981214]]\n",
      "\n",
      "a_fold:\n",
      "[[1.4328878  0.44698645 0.41960118]\n",
      " [0.44698645 1.63689837 0.24465332]\n",
      " [0.41960118 0.24465332 1.67981214]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Folding the flattened value recovers the original matrix.\\n')\n",
    "a_fold = a_pattern.fold(a_flat, free=False)\n",
    "print('a:\\n{}\\n'.format(a))\n",
    "print('a_fold:\\n{}\\n'.format(a_fold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, flattening and folding perform checks to make sure the result is a valid instance of the parameter type -- in this case, a symmetric positive definite matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The diagonal of a positive semi-definite matrix must not be less than 0, and folding checks this when validate=True, which it is by default:\n",
      "\n",
      "A bad folded value: [-1  0  0  0  0  0  0  0  0]\n",
      "Folding with a_pattern raised the following ValueError:\n",
      "Diagonal is less than the lower bound 0.0.\n",
      "\n",
      "If validate is false, folding will produce an invalid matrix without an error:\n",
      "\n",
      "Folding a non-pd matrix with validate=False:\n",
      "[[-1  0  0]\n",
      " [ 0  0  0]\n",
      " [ 0  0  0]]\n",
      "\n",
      "However, it will not produce a matrix of the wrong shape even when validate is False:\n",
      "\n",
      "A very bad folded value: [1 0 0].\n",
      "Folding with a_pattern raised the following ValueError:\n",
      "Wrong length for PSDSymmetricMatrix flat value.\n"
     ]
    }
   ],
   "source": [
    "print('The diagonal of a positive semi-definite matrix must not be less',\n",
    "      'than 0, and folding checks this when validate=True, which it is by default:\\n')\n",
    "a_flat_bad = np.array([-1, 0, 0,  0, 0, 0,  0, 0, 0])\n",
    "print('A bad folded value: {}'.format(a_flat_bad))\n",
    "try:\n",
    "    a_fold_bad = a_pattern.fold(a_flat_bad, free=False)\n",
    "except ValueError as err:\n",
    "    print('Folding with a_pattern raised the following ValueError:\\n{}'.format(err))\n",
    "\n",
    "print('\\nIf validate is false, folding will produce an invalid matrix without an error:\\n')\n",
    "a_fold_bad = a_pattern.fold(a_flat_bad, free=False, validate=False)\n",
    "print('Folding a non-pd matrix with validate=False:\\n{}'.format(a_fold_bad))\n",
    "\n",
    "print('\\nHowever, it will not produce a matrix of the wrong shape even when validate is False:\\n')\n",
    "a_flat_very_bad = np.array([1, 0, 0])\n",
    "print('A very bad folded value: {}.'.format(a_flat_very_bad))\n",
    "try:\n",
    "    a_fold_very_bad = a_pattern.fold(a_flat_very_bad, free=False, validate=False)\n",
    "except ValueError as err:\n",
    "    print('Folding with a_pattern raised the following ValueError:\\n{}'.format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free flattening and folding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary flattening converts a 3x3 symmetric PSD matrix into a 9-d vector.  However, as seen above, not every 9-d vector is a valid 3x3 symmetric positive definite matrix.  It is useful to have an \"free\" flattened representation of a parameter, where every finite value of the free flattened vector corresponds is guaranteed valid.\n",
    "\n",
    "To accomplish this for a symmetric positive definite matrix, we consider the Cholesky decomposition $A_{chol}$. This is an lower-triangular matrix with positive diagonal entries such that $A = A_{chol} A_{chol}^T$.  By taking the log of the diagonal of $A_{chol}$ and stacking the non-zero entries, we can construct a 6-d vector, every value of which corresponds to a symmetric PSD matrix.\n",
    "\n",
    "The details aren't important here, as `paragami` takes care of the transformation behind the scenes with the option `free=True`.  We denote the flattened $A$ in the free parameterization as $A_{freeflat}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The free flat value a_freeflat is not immediately recognizable as a.\n",
      "\n",
      "a:\n",
      "[[1.4328878  0.44698645 0.41960118]\n",
      " [0.44698645 1.63689837 0.24465332]\n",
      " [0.41960118 0.24465332 1.67981214]]\n",
      "\n",
      "a_freeflat:\n",
      "[0.17984592 0.373412   0.20188579 0.35053437 0.09296299 0.21857738]\n",
      "\n",
      "However, it transforms correctly back to a when folded.\n",
      "\n",
      "a_fold:\n",
      "[[1.4328878  0.44698645 0.41960118]\n",
      " [0.44698645 1.63689837 0.24465332]\n",
      " [0.41960118 0.24465332 1.67981214]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('The free flat value a_freeflat is not immediately recognizable as a.\\n')\n",
    "a_freeflat = a_pattern.flatten(a, free=True)\n",
    "print('a:\\n{}\\n'.format(a))\n",
    "print('a_freeflat:\\n{}\\n'.format(a_freeflat))\n",
    "\n",
    "print('However, it transforms correctly back to a when folded.\\n')\n",
    "a_freefold = a_pattern.fold(a_freeflat, free=True)\n",
    "print('a_fold:\\n{}\\n'.format(a_freefold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any length-six vector will free fold back to a valid PSD matrix up to floating point error.  Let's draw 100 random vectors, fold them, and check that this is true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Draw random free vectors and confirm that they are positive semi definite.\n",
    "def assert_is_pd(mat):\n",
    "    eigvals = np.linalg.eigvals(mat)\n",
    "    assert np.min(eigvals) >= -1e-8\n",
    "for draw in range(100):\n",
    "    a_rand_freeflat = np.random.normal(scale=2, size=(6, ))\n",
    "    a_rand_fold = a_pattern.fold(a_rand_freeflat, free=True)\n",
    "    assert_is_pd(a_rand_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using flattening and folding for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are interested in optimizing some function of $A$, say, a normal model in which the data $x_n \\sim \\mathcal{N}(0, A)$.  Specifically, Let the data be $X = \\left(x_1, ..., x_N\\right)$, where $x_n \\in \\mathbb{R}^3$, and write a loss function as\n",
    "\n",
    "$$\n",
    "\\ell\\left(X, A\\right) =\n",
    "    -\\sum_{n=1}^N \\log P(x_n | A) =\n",
    "    \\frac{1}{2}\\sum_{n=1}^N \\left(x_n^T A^{-1} x_n - \\log|A|\\right) \n",
    "$$\n",
    "\n",
    "Let's simulate some data under this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1000, 3)\n",
      "Loss at true parameter: 2392.751922600241\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_obs = 1000\n",
    "\n",
    "# True value of A\n",
    "true_a = np.eye(3) * np.diag(np.array([1, 2, 3])) + np.random.random((3, 3)) * 0.1\n",
    "true_a = 0.5 * (true_a + true_a.T)\n",
    "\n",
    "# Data\n",
    "def draw_data(num_obs, true_a):\n",
    "    return np.random.multivariate_normal(\n",
    "        mean=np.zeros(3), cov=true_a, size=(num_obs, ))\n",
    "\n",
    "x = draw_data(num_obs, true_a)\n",
    "print('X shape: {}'.format(x.shape))\n",
    "\n",
    "def get_loss(x, a):\n",
    "    num_obs = x.shape[0]\n",
    "    a_inv = np.linalg.inv(a)\n",
    "    a_det_sign, a_log_det = np.linalg.slogdet(a)\n",
    "    assert a_det_sign > 0\n",
    "    return 0.5 * (np.einsum('ni,ij,nj', x, a_inv, x) + num_obs * a_log_det)\n",
    "\n",
    "print('Loss at true parameter: {}'.format(get_loss(x, true_a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to minimize the function `loss` using tools like `scipy.optimize.minimize`.  Standard optimization functions take vectors, not matrices, as input, and often require the vector to take valid values in the entire domain.\n",
    "\n",
    "As-written, our loss function takes a positive definite matrix as an input.  We can wrap the loss as a funciton of the free flattened value using the `paragami.FlattenedFunction` class.  That is, we want to define a function $\\ell_{freeflat}$ so that\n",
    "\n",
    "$$\n",
    "\\ell_{freeflat}(X, A_{freeflat}) = \\ell(X, A).\n",
    "$$\n",
    "\n",
    "\n",
    "The resulting function can be passed directly to `autograd` and `scipy.optimize`, and we can estimate\n",
    "\n",
    "$$\n",
    "\\hat{A}_{freeflat} := \\mathrm{argmin}_{A_{freeflat}} \\ell_{freeflat}(X, A_{freeflat})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two losses are the same when evalated on the folded and flat values:\n",
      "\n",
      "Original loss:\t\t2392.751922600241\n",
      "Free-flattened loss: \t2392.751922600241\n",
      "\n",
      "Now, use the flattened function to optimize with autograd.\n",
      "\n",
      "Optimization successful: False\n",
      "Optimal value: 2390.6463320487683\n"
     ]
    }
   ],
   "source": [
    "# The arguments mean we're flatting the function get_loss, using\n",
    "# the pattern a_pattern, with free parameterization, and the paramater\n",
    "# is the second one (argnums uses 0-indexing like autograd).\n",
    "get_freeflat_loss = paragami.FlattenedFunction(\n",
    "    original_fun=get_loss, patterns=a_pattern, free=True, argnums=1)\n",
    "\n",
    "print('The two losses are the same when evalated on the folded and flat values:\\n')\n",
    "print('Original loss:\\t\\t{}'.format(get_loss(x, true_a)))\n",
    "true_a_freeflat = a_pattern.flatten(true_a, free=True)\n",
    "print('Free-flattened loss: \\t{}'.format(\n",
    "    get_freeflat_loss(x, true_a_freeflat)))\n",
    "\n",
    "print('\\nNow, use the flattened function to optimize with autograd.\\n')\n",
    "\n",
    "get_freeflat_loss_grad = autograd.grad(get_freeflat_loss, argnum=1)\n",
    "get_freeflat_loss_hessian = autograd.hessian(get_freeflat_loss, argnum=1)\n",
    "\n",
    "def get_optimum(x):\n",
    "    loss_opt = osp.optimize.minimize(\n",
    "        method='trust-ncg',\n",
    "        x0=np.zeros(a_pattern.flat_length(free=True)),\n",
    "        fun=lambda par: get_freeflat_loss(x, par),\n",
    "        jac=lambda par: get_freeflat_loss_grad(x, par),\n",
    "        hess=lambda par: get_freeflat_loss_hessian(x, par),\n",
    "        options={'gtol': 1e-8, 'disp': False})\n",
    "    return loss_opt\n",
    "\n",
    "loss_opt = get_optimum(x)\n",
    "print('Optimization successful: {}\\nOptimal value: {}'.format(\n",
    "    loss_opt.success, loss_opt.fun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization was in the free flattened space, so to get the optimal value of $A$ we must fold it.   We can see that the optimal value is close to the true value of $A$, though it differs due to randomness in $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True a:\n",
      "[[1.03745401 0.07746864 0.03950388]\n",
      " [0.07746864 2.01560186 0.05110853]\n",
      " [0.03950388 0.05110853 3.0601115 ]]\n",
      "\n",
      "Optimal a:\n",
      "[[ 1.0688328   0.07771763  0.04877013]\n",
      " [ 0.07771763  1.89393526 -0.03098498]\n",
      " [ 0.04877013 -0.03098498  2.94476044]]\n"
     ]
    }
   ],
   "source": [
    "optimal_freeflat_a = loss_opt.x\n",
    "optimal_a = a_pattern.fold(optimal_freeflat_a, free=True)\n",
    "print('True a:\\n{}\\n\\nOptimal a:\\n{}'.format(true_a, optimal_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using flattening and folding for frequentist uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wanted to use the Hessian of the objective (the observed Fisher information) to estimate a frequentist confidence region for $A$.  In standard notation, covariance is of a vector, so we can write what we want in terms of $A_{flat}$ as $\\mathrm{Cov}(A_{flat})$.\n",
    "The covariance between two elements of $A_{flat}$ corresponds to that between two elements of $A$.  For example, using the notation given above,\n",
    "\n",
    "$$\n",
    "\\mathrm{Cov}(a_{flat,1}, a_{flat,2}) = \\mathrm{Cov}(a_{11}, a_{12}) = \\mathrm{Cov}(a_{11}, a_{21})\\\\\n",
    "\\mathrm{Var}(a_{flat,4}) = \\mathrm{Var}(a_{21}) = \\mathrm{Var}(a_{12}),\n",
    "$$\n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use the observed Fisher information of $\\ell_{freeflat}$ and the Delta method to estimate $\\mathrm{Cov}(A_{flat})$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{Cov}(A_{freeflat}) &\\approx\n",
    "-\\left( \\left.\n",
    "\\frac{\\partial^2 \\ell_{freeflat}}{\\partial A_{freeflat} \\partial A_{freeflat}^T}\n",
    "\\right|_{\\hat{A}_{freeflat}} \\right)^{-1}\n",
    "&\\quad\\textrm{(Fisher information)}\n",
    "\\\\\n",
    "\\mathrm{Cov}(A_{free}) &\\approx\n",
    "\\left(\\frac{d A_{free}}{dA_{freeflat}^T}\\right)\n",
    "\\mathrm{Cov}(A_{freeflat})\n",
    "\\left(\\frac{d A_{free}}{dA_{freeflat}^T}\\right)^T\n",
    "&\\quad\\textrm{(Delta method)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hessian required for the covariance can be calculated directly using ``autograd``.  (Note that the loss is the negative of the log likelihood.)  The shape is, of course, the size of $A_{freeflat}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the Fisher information amtrix is (6, 6).\n"
     ]
    }
   ],
   "source": [
    "fisher_info = -1 * get_freeflat_loss_hessian(x, loss_opt.x)\n",
    "print(\"The shape of the Fisher information amtrix is {}.\".format(fisher_info.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jacobian matrix $\\frac{d A_{free}}{dA_{freeflat}^T}$ of the \"unfreeing transform\" $A_{free} = A_{free}(A_{freeflat})$ is provided by ``paragami`` as a function of the *folded* parameter.  Following standard notation for Jacobian matrices, the rows correspond to $A_{flat}$, the output of the unfreeing transform, and the columns correspond to $A_{freeflat}$, the input to the unfreeing transform.\n",
    "\n",
    "By default this Jacobian matrix is sparse (in large problems, most flat parameters are independent of most free flat parameters), but a dense matrix is fine in this small problem, so we use ``sparse=False``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the Jacobian matrix is (9, 6).\n"
     ]
    }
   ],
   "source": [
    "freeing_jac = a_pattern.unfreeing_jacobian(optimal_a, sparse=False)\n",
    "print(\"The shape of the Jacobian matrix is {}.\".format(freeing_jac.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plug in to estimate the covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the covariance matrix is (9, 9).\n"
     ]
    }
   ],
   "source": [
    "# Estimate the covariance of the flattened value using the Hessian at the optimum.\n",
    "a_flattened_cov = -1 * freeing_jac @ np.linalg.solve(fisher_info, freeing_jac.T)\n",
    "print(\"The shape of the covariance matrix is {}.\".format(a_flattened_cov.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shape of $\\mathrm{Cov}(A_{flat})$ is inconvenient because it's not obvious visually which entry of the flattened vector corresponds to which element of $A$.  Again, we can use folding to put the estimated marginal standard deviations in a readable shape.\n",
    "\n",
    "Because the result is not a valid covariance matrix, and we are just using the pattern for its shape, we set `validate` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The marginal standard deviations of the elements of A:\n",
      "[[0.04779966 0.0450593  0.05612339]\n",
      " [0.0450593  0.08469936 0.07468698]\n",
      " [0.05612339 0.07468698 0.13169369]]\n"
     ]
    }
   ],
   "source": [
    "a_pattern.verify = False\n",
    "a_sd = a_pattern.fold(np.sqrt(np.diag(a_flattened_cov)), free=False, validate=False)\n",
    "print('The marginal standard deviations of the elements of A:\\n{}'.format(a_sd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we can compare this estimated covariance with the variability incurred by drawing new datasets and re-optimizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2436.353389\n",
      "         Iterations: 10\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 11\n",
      "         Hessian evaluations: 11\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2521.403091\n",
      "         Iterations: 10\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 11\n",
      "         Hessian evaluations: 11\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2411.536745\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2428.205455\n",
      "         Iterations: 10\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 11\n",
      "         Hessian evaluations: 11\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2433.237388\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2448.436581\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2399.896062\n",
      "         Iterations: 7\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 8\n",
      "         Hessian evaluations: 8\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2416.088757\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2360.815295\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2446.885338\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 2379.159759\n",
      "         Iterations: 8\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 8\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2415.877705\n",
      "         Iterations: 7\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 8\n",
      "         Hessian evaluations: 8\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2418.903720\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2444.902800\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2411.118294\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2405.815304\n",
      "         Iterations: 19\n",
      "         Function evaluations: 21\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2470.832959\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2490.473177\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2454.758837\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2449.623134\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2346.018777\n",
      "         Iterations: 10\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 11\n",
      "         Hessian evaluations: 11\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2412.289789\n",
      "         Iterations: 23\n",
      "         Function evaluations: 25\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2438.679359\n",
      "         Iterations: 10\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 11\n",
      "         Hessian evaluations: 11\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2456.233810\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2429.388910\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2495.618453\n",
      "         Iterations: 24\n",
      "         Function evaluations: 26\n",
      "         Gradient evaluations: 11\n",
      "         Hessian evaluations: 11\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2461.927193\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2476.607856\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2399.308842\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2372.307718\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2408.679287\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2429.877491\n",
      "         Iterations: 23\n",
      "         Function evaluations: 25\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2386.280209\n",
      "         Iterations: 7\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 8\n",
      "         Hessian evaluations: 8\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2442.755084\n",
      "         Iterations: 24\n",
      "         Function evaluations: 26\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2421.800660\n",
      "         Iterations: 10\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 11\n",
      "         Hessian evaluations: 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2455.679183\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2347.882599\n",
      "         Iterations: 10\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 11\n",
      "         Hessian evaluations: 11\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2397.646981\n",
      "         Iterations: 7\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 8\n",
      "         Hessian evaluations: 8\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 2361.797259\n",
      "         Iterations: 10\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2483.591506\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2407.758689\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2447.847174\n",
      "         Iterations: 10\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 11\n",
      "         Hessian evaluations: 11\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2477.780698\n",
      "         Iterations: 10\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 11\n",
      "         Hessian evaluations: 11\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2493.725205\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2464.312908\n",
      "         Iterations: 7\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 8\n",
      "         Hessian evaluations: 8\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2437.111760\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2420.162947\n",
      "         Iterations: 22\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2385.969293\n",
      "         Iterations: 22\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2475.238002\n",
      "         Iterations: 23\n",
      "         Function evaluations: 25\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2349.089449\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2292.952033\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2377.053528\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2384.590582\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2411.250748\n",
      "         Iterations: 10\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 11\n",
      "         Hessian evaluations: 11\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2499.465631\n",
      "         Iterations: 22\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2449.063751\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2413.983524\n",
      "         Iterations: 7\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 8\n",
      "         Hessian evaluations: 8\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2429.797073\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2413.922148\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2438.994048\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2455.225504\n",
      "         Iterations: 11\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 12\n",
      "         Hessian evaluations: 12\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2467.551814\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2460.752212\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 2442.447763\n",
      "         Iterations: 23\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 11\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2410.506001\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2412.987117\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2456.359671\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2426.403216\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2493.068297\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2413.428077\n",
      "         Iterations: 22\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2410.851033\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2388.782891\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2411.738809\n",
      "         Iterations: 22\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2377.354783\n",
      "         Iterations: 8\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2396.717612\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2421.427025\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 10\n",
      "         Hessian evaluations: 10\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: 2433.265419\n",
      "         Iterations: 22\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 9\n",
      "         Hessian evaluations: 9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2117a4528f14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_sims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnew_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnew_loss_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_optimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0moptimal_a_draws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_pattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_loss_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-c581369181ca>\u001b[0m in \u001b[0;36mget_optimum\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_freeflat_loss_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mhess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_freeflat_loss_hessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             options={'gtol': 1e-8, 'disp': True})\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_opt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'trust-ncg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         return _minimize_trust_ncg(fun, x0, args, jac, hess, hessp,\n\u001b[0;32m--> 501\u001b[0;31m                                    callback=callback, **options)\n\u001b[0m\u001b[1;32m    502\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'trust-krylov'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         return _minimize_trust_krylov(fun, x0, args, jac, hess, hessp,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/optimize/_trustregion_ncg.py\u001b[0m in \u001b[0;36m_minimize_trust_ncg\u001b[0;34m(fun, x0, args, jac, hess, hessp, **trust_region_options)\u001b[0m\n\u001b[1;32m     39\u001b[0m     return _minimize_trust_region(fun, x0, args=args, jac=jac, hess=hess,\n\u001b[1;32m     40\u001b[0m                                   \u001b[0mhessp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhessp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubproblem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCGSteihaugSubproblem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                                   **trust_region_options)\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/optimize/_trustregion.py\u001b[0m in \u001b[0;36m_minimize_trust_region\u001b[0;34m(fun, x0, args, jac, hess, hessp, subproblem, initial_trust_radius, max_trust_radius, eta, gtol, maxiter, disp, return_all, callback, inexact, **unknown_options)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# has reached the trust region boundary or not.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhits_boundary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrust_radius\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinAlgError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mwarnflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/optimize/_trustregion_ncg.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, trust_radius)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;31m# do an iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mBd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhessp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mdBd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdBd\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/optimize/_trustregion.py\u001b[0m in \u001b[0;36mhessp\u001b[0;34m(self, p)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hessp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/optimize/_trustregion.py\u001b[0m in \u001b[0;36mhess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;34m\"\"\"Value of hessian of objective function at current iteration.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_h\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-c581369181ca>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(par)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_freeflat_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_freeflat_loss_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mhess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_freeflat_loss_hessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             options={'gtol': 1e-8, 'disp': True})\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_opt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/autograd/wrap_util.py\u001b[0m in \u001b[0;36mnary_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0munary_operator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munary_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnary_op_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnary_op_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnary_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnary_operator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/autograd/differential_operators.py\u001b[0m in \u001b[0;36mhessian\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;34m\"Returns a function that computes the exact Hessian.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0munary_to_nary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/autograd/wrap_util.py\u001b[0m in \u001b[0;36mnary_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0munary_operator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munary_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnary_op_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnary_op_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnary_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnary_operator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/autograd/differential_operators.py\u001b[0m in \u001b[0;36mjacobian\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mjacobian_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mans_vspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans_vspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandard_basis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjacobian_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0munary_to_nary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/autograd/numpy/numpy_wrapper.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# this code is basically copied from numpy/core/shape_base.py's stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'need at least one array to stack'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/autograd/numpy/numpy_wrapper.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# this code is basically copied from numpy/core/shape_base.py's stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'need at least one array to stack'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/autograd/core.py\u001b[0m in \u001b[0;36mvjp\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mvspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/autograd/core.py\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(g, end_node)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoposort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mingrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0moutgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_outgrads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/autograd/core.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \"VJP of {} wrt argnum 0 not defined\".format(fun.__name__))\n\u001b[1;32m     60\u001b[0m             \u001b[0mvjp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjpfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0margnum_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnum_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_sims = 100\n",
    "optimal_a_draws = np.empty((num_sims, ) + true_a.shape)\n",
    "for sim in range(num_sims):\n",
    "    new_x = draw_data(num_obs, true_a)\n",
    "    new_loss_opt = get_optimum(new_x)\n",
    "    optimal_a_draws[sim] = a_pattern.fold(new_loss_opt.x, free=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Actual standard deviation:\\n{}'.format(np.std(optimal_a_draws, axis=0)))\n",
    "print('Estimated standard deviation:\\n{}'.format(a_sd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
