{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folding and Unfolding Example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autograd\n",
    "from autograd import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import paragami\n",
    "\n",
    "# Use the original scipy (\"osp\") for functions we don't need to differentiate.\n",
    "# When using scipy functions in functions that are passed to autograd,\n",
    "# use autograd.scipy instead.\n",
    "import scipy as osp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will consider flattening and folding a simple symmetric positive semi-definite matrix:\n",
    "\n",
    "$$\n",
    "A = \\left[\n",
    "\\begin{matrix}\n",
    "a_{11} & a_{12} & a_{13}  \\\\\n",
    "a_{21} & a_{22} & a_{23}  \\\\\n",
    "a_{31} & a_{32} & a_{33}  \\\\\n",
    "\\end{matrix}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "Of course, symmetry and positive semi-definiteness impose constraints on the entries $a_{ij}$ of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening and folding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first consider how to represent $A$ as a vector, which we call simply *flattening*, and then as an unconstrained vector, which we call *free flattening*.\n",
    "\n",
    "When a parameter is flattened, it is simply re-shaped as a vector.  Every number that was in the original parameter will occur exactly once in the flattened shape.  (In the present case of a matrix, this is exactly the same as ``np.flatten``.)\n",
    "\n",
    "$$\n",
    "A = \\left[\n",
    "\\begin{matrix}\n",
    "a_{11} & a_{12} & a_{13}  \\\\\n",
    "a_{21} & a_{22} & a_{23}  \\\\\n",
    "a_{31} & a_{32} & a_{33}  \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\xrightarrow{flatten}\n",
    "A_{flat} = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "a_{flat,1} \\\\\n",
    "a_{flat,2} \\\\\n",
    "a_{flat,3} \\\\\n",
    "a_{flat,4} \\\\\n",
    "a_{flat,5} \\\\\n",
    "a_{flat,6} \\\\\n",
    "a_{flat,7} \\\\\n",
    "a_{flat,8} \\\\\n",
    "a_{flat,9} \\\\\n",
    "\\end{matrix}\\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "a_{11} \\\\\n",
    "a_{12} \\\\\n",
    "a_{13} \\\\\n",
    "a_{21} \\\\\n",
    "a_{22} \\\\\n",
    "a_{23} \\\\\n",
    "a_{31} \\\\\n",
    "a_{32} \\\\\n",
    "a_{33} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to and from $A$ and $A_{flat}$ can be done with the `flatten` method of a `paragami.PSDSymmetricMatrixPattern` pattern.  \n",
    "\n",
    "For the moment, because we are flattening, not free flattening, we use the option `free=False`.  We will discuss the `free=True` option shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, a_flat contains the elements of a exactly as shown in the formula above.\n",
      "\n",
      "a:\n",
      "[[1.848847   0.90130543 0.26086451]\n",
      " [0.90130543 1.29680299 0.66583614]\n",
      " [0.26086451 0.66583614 1.6963313 ]]\n",
      "\n",
      "a_flat:\n",
      "[1.848847   0.90130543 0.26086451 0.90130543 1.29680299 0.66583614\n",
      " 0.26086451 0.66583614 1.6963313 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A sample positive semi-definite matrix.\n",
    "a = np.eye(3) + np.random.random((3, 3))\n",
    "a = 0.5 * (a + a.T)\n",
    "\n",
    "# Define a pattern and fold.\n",
    "a_pattern = paragami.PSDSymmetricMatrixPattern(size=3)\n",
    "a_flat = a_pattern.flatten(a, free=False)\n",
    "\n",
    "print('Now, a_flat contains the elements of a exactly as shown in the formula above.\\n')\n",
    "print('a:\\n{}\\n'.format(a))\n",
    "print('a_flat:\\n{}\\n'.format(a_flat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also convert from $A_{flat}$ back to $A$ by 'folding'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folding the flattened value recovers the original matrix.\n",
      "\n",
      "a:\n",
      "[[1.848847   0.90130543 0.26086451]\n",
      " [0.90130543 1.29680299 0.66583614]\n",
      " [0.26086451 0.66583614 1.6963313 ]]\n",
      "\n",
      "a_fold:\n",
      "[[1.848847   0.90130543 0.26086451]\n",
      " [0.90130543 1.29680299 0.66583614]\n",
      " [0.26086451 0.66583614 1.6963313 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Folding the flattened value recovers the original matrix.\\n')\n",
    "a_fold = a_pattern.fold(a_flat, free=False)\n",
    "print('a:\\n{}\\n'.format(a))\n",
    "print('a_fold:\\n{}\\n'.format(a_fold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, flattening and folding perform checks to make sure the result is a valid instance of the parameter type -- in this case, a symmetric positive definite matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The diagonal of a positive semi-definite matrix must not be less than 0, and folding checks this when validate=True, which it is by default:\n",
      "\n",
      "A bad folded value: [-1  0  0  0  0  0  0  0  0]\n",
      "Folding with a_pattern raised the following ValueError:\n",
      "Diagonal is less than the lower bound 0.0.\n",
      "\n",
      "If validate is false, folding will produce an invalid matrix without an error:\n",
      "\n",
      "Folding a non-pd matrix with validate=False:\n",
      "[[-1  0  0]\n",
      " [ 0  0  0]\n",
      " [ 0  0  0]]\n",
      "\n",
      "However, it will not produce a matrix of the wrong shape even when validate is False:\n",
      "\n",
      "A very bad folded value: [1 0 0].\n",
      "Folding with a_pattern raised the following ValueError:\n",
      "Wrong length for PSDSymmetricMatrix flat value.\n"
     ]
    }
   ],
   "source": [
    "print('The diagonal of a positive semi-definite matrix must not be less',\n",
    "      'than 0, and folding checks this when validate=True, which it is by default:\\n')\n",
    "a_flat_bad = np.array([-1, 0, 0,  0, 0, 0,  0, 0, 0])\n",
    "print('A bad folded value: {}'.format(a_flat_bad))\n",
    "try:\n",
    "    a_fold_bad = a_pattern.fold(a_flat_bad, free=False)\n",
    "except ValueError as err:\n",
    "    print('Folding with a_pattern raised the following ValueError:\\n{}'.format(err))\n",
    "\n",
    "print('\\nIf validate is false, folding will produce an invalid matrix without an error:\\n')\n",
    "a_fold_bad = a_pattern.fold(a_flat_bad, free=False, validate=False)\n",
    "print('Folding a non-pd matrix with validate=False:\\n{}'.format(a_fold_bad))\n",
    "\n",
    "print('\\nHowever, it will not produce a matrix of the wrong shape even when validate is False:\\n')\n",
    "a_flat_very_bad = np.array([1, 0, 0])\n",
    "print('A very bad folded value: {}.'.format(a_flat_very_bad))\n",
    "try:\n",
    "    a_fold_very_bad = a_pattern.fold(a_flat_very_bad, free=False, validate=False)\n",
    "except ValueError as err:\n",
    "    print('Folding with a_pattern raised the following ValueError:\\n{}'.format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free flattening and folding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary flattening converts a 3x3 symmetric PSD matrix into a 9-d vector.  However, as seen above, not every 9-d vector is a valid 3x3 symmetric positive definite matrix.  It is useful to have an \"free\" flattened representation of a parameter, where every finite value of the free flattened vector corresponds is guaranteed valid.\n",
    "\n",
    "To accomplish this for a symmetric positive definite matrix, we consider the Cholesky decomposition $A_{chol}$. This is an lower-triangular matrix with positive diagonal entries such that $A = A_{chol} A_{chol}^T$.  By taking the log of the diagonal of $A_{chol}$ and stacking the non-zero entries, we can construct a 6-d vector, every value of which corresponds to a symmetric PSD matrix.\n",
    "\n",
    "The details aren't important here, as `paragami` takes care of the transformation behind the scenes with the option `free=True`.  We denote the flattened $A$ in the free parameterization as $A_{freeflat}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The free flat value a_freeflat is not immediately recognizable as a.\n",
      "\n",
      "a:\n",
      "[[1.848847   0.90130543 0.26086451]\n",
      " [0.90130543 1.29680299 0.66583614]\n",
      " [0.26086451 0.66583614 1.6963313 ]]\n",
      "\n",
      "a_freeflat:\n",
      "[ 0.3072811   0.66285952 -0.07691356  0.19185119  0.58173139  0.13923728]\n",
      "\n",
      "However, it transforms correctly back to a when folded.\n",
      "\n",
      "a_fold:\n",
      "[[1.848847   0.90130543 0.26086451]\n",
      " [0.90130543 1.29680299 0.66583614]\n",
      " [0.26086451 0.66583614 1.6963313 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('The free flat value a_freeflat is not immediately recognizable as a.\\n')\n",
    "a_freeflat = a_pattern.flatten(a, free=True)\n",
    "print('a:\\n{}\\n'.format(a))\n",
    "print('a_freeflat:\\n{}\\n'.format(a_freeflat))\n",
    "\n",
    "print('However, it transforms correctly back to a when folded.\\n')\n",
    "a_freefold = a_pattern.fold(a_freeflat, free=True)\n",
    "print('a_fold:\\n{}\\n'.format(a_freefold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any length-six vector will free fold back to a valid PSD matrix up to floating point error.  Let's draw 100 random vectors, fold them, and check that this is true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Draw random free vectors and confirm that they are positive semi definite.\n",
    "def assert_is_pd(mat):\n",
    "    eigvals = np.linalg.eigvals(mat)\n",
    "    assert np.min(eigvals) >= -1e-8\n",
    "for draw in range(100):\n",
    "    a_rand_freeflat = np.random.normal(scale=2, size=(6, ))\n",
    "    a_rand_fold = a_pattern.fold(a_rand_freeflat, free=True)\n",
    "    assert_is_pd(a_rand_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using flattening and folding for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are interested in optimizing some function of $A$, say, a normal model in which the data $x_n \\sim \\mathcal{N}(0, A)$.  Specifically, Let the data be $X = \\left(x_1, ..., x_N\\right)$, where $x_n \\in \\mathbb{R}^3$, and write a loss function as\n",
    "\n",
    "$$\n",
    "\\ell\\left(X, A\\right) =\n",
    "    -\\sum_{n=1}^N \\log P(x_n | A) =\n",
    "    \\frac{1}{2}\\sum_{n=1}^N \\left(x_n^T A^{-1} x_n - \\log|A|\\right) \n",
    "$$\n",
    "\n",
    "Let's simulate some data under this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1000, 3)\n",
      "Loss at true parameter: 2392.751922600241\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_obs = 1000\n",
    "\n",
    "# True value of A\n",
    "true_a = np.eye(3) * np.diag(np.array([1, 2, 3])) + np.random.random((3, 3)) * 0.1\n",
    "true_a = 0.5 * (true_a + true_a.T)\n",
    "\n",
    "# Data\n",
    "def draw_data(num_obs, true_a):\n",
    "    return np.random.multivariate_normal(\n",
    "        mean=np.zeros(3), cov=true_a, size=(num_obs, ))\n",
    "\n",
    "x = draw_data(num_obs, true_a)\n",
    "print('X shape: {}'.format(x.shape))\n",
    "\n",
    "def get_loss(x, a):\n",
    "    num_obs = x.shape[0]\n",
    "    a_inv = np.linalg.inv(a)\n",
    "    a_det_sign, a_log_det = np.linalg.slogdet(a)\n",
    "    assert a_det_sign > 0\n",
    "    return 0.5 * (np.einsum('ni,ij,nj', x, a_inv, x) + num_obs * a_log_det)\n",
    "\n",
    "print('Loss at true parameter: {}'.format(get_loss(x, true_a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to minimize the function `loss` using tools like `scipy.optimize.minimize`.  Standard optimization functions take vectors, not matrices, as input, and often require the vector to take valid values in the entire domain.\n",
    "\n",
    "As-written, our loss function takes a positive definite matrix as an input.  We can wrap the loss as a funciton of the free flattened value using the `paragami.FlattenedFunction` class.  That is, we want to define a function $\\ell_{freeflat}$ so that\n",
    "\n",
    "$$\n",
    "\\ell_{freeflat}(X, A_{freeflat}) = \\ell(X, A).\n",
    "$$\n",
    "\n",
    "\n",
    "The resulting function can be passed directly to `autograd` and `scipy.optimize`, and we can estimate\n",
    "\n",
    "$$\n",
    "\\hat{A}_{freeflat} := \\mathrm{argmin}_{A_{freeflat}} \\ell_{freeflat}(X, A_{freeflat})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two losses are the same when evalated on the folded and flat values:\n",
      "\n",
      "Original loss:\t\t2392.751922600241\n",
      "Free-flattened loss: \t2392.751922600241\n",
      "\n",
      "Now, use the flattened function to optimize with autograd.\n",
      "\n",
      "Optimization successful: False\n",
      "Optimal value: 2390.6463320487683\n"
     ]
    }
   ],
   "source": [
    "# The arguments mean we're flatting the function get_loss, using\n",
    "# the pattern a_pattern, with free parameterization, and the paramater\n",
    "# is the second one (argnums uses 0-indexing like autograd).\n",
    "get_freeflat_loss = paragami.FlattenedFunction(\n",
    "    original_fun=get_loss, patterns=a_pattern, free=True, argnums=1)\n",
    "\n",
    "print('The two losses are the same when evalated on the folded and flat values:\\n')\n",
    "print('Original loss:\\t\\t{}'.format(get_loss(x, true_a)))\n",
    "true_a_freeflat = a_pattern.flatten(true_a, free=True)\n",
    "print('Free-flattened loss: \\t{}'.format(\n",
    "    get_freeflat_loss(x, true_a_freeflat)))\n",
    "\n",
    "print('\\nNow, use the flattened function to optimize with autograd.\\n')\n",
    "\n",
    "get_freeflat_loss_grad = autograd.grad(get_freeflat_loss, argnum=1)\n",
    "get_freeflat_loss_hessian = autograd.hessian(get_freeflat_loss, argnum=1)\n",
    "\n",
    "def get_optimum(x):\n",
    "    loss_opt = osp.optimize.minimize(\n",
    "        method='trust-ncg',\n",
    "        x0=np.zeros(a_pattern.flat_length(free=True)),\n",
    "        fun=lambda par: get_freeflat_loss(x, par),\n",
    "        jac=lambda par: get_freeflat_loss_grad(x, par),\n",
    "        hess=lambda par: get_freeflat_loss_hessian(x, par),\n",
    "        options={'gtol': 1e-8, 'disp': False})\n",
    "    return loss_opt\n",
    "\n",
    "loss_opt = get_optimum(x)\n",
    "print('Optimization successful: {}\\nOptimal value: {}'.format(\n",
    "    loss_opt.success, loss_opt.fun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization was in the free flattened space, so to get the optimal value of $A$ we must fold it.   We can see that the optimal value is close to the true value of $A$, though it differs due to randomness in $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True a:\n",
      "[[1.03745401 0.07746864 0.03950388]\n",
      " [0.07746864 2.01560186 0.05110853]\n",
      " [0.03950388 0.05110853 3.0601115 ]]\n",
      "\n",
      "Optimal a:\n",
      "[[ 1.0688328   0.07771763  0.04877013]\n",
      " [ 0.07771763  1.89393526 -0.03098498]\n",
      " [ 0.04877013 -0.03098498  2.94476044]]\n"
     ]
    }
   ],
   "source": [
    "optimal_freeflat_a = loss_opt.x\n",
    "optimal_a = a_pattern.fold(optimal_freeflat_a, free=True)\n",
    "print('True a:\\n{}\\n\\nOptimal a:\\n{}'.format(true_a, optimal_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using flattening and folding for frequentist uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wanted to use the Hessian of the objective (the observed Fisher information) to estimate a frequentist confidence region for $A$.  In standard notation, covariance is of a vector, so we can write what we want in terms of $A_{flat}$ as $\\mathrm{Cov}(A_{flat})$.\n",
    "The covariance between two elements of $A_{flat}$ corresponds to that between two elements of $A$.  For example, using the notation given above,\n",
    "\n",
    "$$\n",
    "\\mathrm{Cov}(a_{flat,1}, a_{flat,2}) = \\mathrm{Cov}(a_{11}, a_{12}) = \\mathrm{Cov}(a_{11}, a_{21})\\\\\n",
    "\\mathrm{Var}(a_{flat,4}) = \\mathrm{Var}(a_{21}) = \\mathrm{Var}(a_{12}),\n",
    "$$\n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use the observed Fisher information of $\\ell_{freeflat}$ and the Delta method to estimate $\\mathrm{Cov}(A_{flat})$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{Cov}(A_{freeflat}) &\\approx\n",
    "-\\left( \\left.\n",
    "\\frac{\\partial^2 \\ell_{freeflat}}{\\partial A_{freeflat} \\partial A_{freeflat}^T}\n",
    "\\right|_{\\hat{A}_{freeflat}} \\right)^{-1}\n",
    "&\\quad\\textrm{(Fisher information)}\n",
    "\\\\\n",
    "\\mathrm{Cov}(A_{free}) &\\approx\n",
    "\\left(\\frac{d A_{free}}{dA_{freeflat}^T}\\right)\n",
    "\\mathrm{Cov}(A_{freeflat})\n",
    "\\left(\\frac{d A_{free}}{dA_{freeflat}^T}\\right)^T\n",
    "&\\quad\\textrm{(Delta method)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hessian required for the covariance can be calculated directly using ``autograd``.  (Note that the loss is the negative of the log likelihood.)  The shape is, of course, the size of $A_{freeflat}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the Fisher information amtrix is (6, 6).\n"
     ]
    }
   ],
   "source": [
    "fisher_info = -1 * get_freeflat_loss_hessian(x, loss_opt.x)\n",
    "print(\"The shape of the Fisher information amtrix is {}.\".format(fisher_info.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jacobian matrix $\\frac{d A_{free}}{dA_{freeflat}^T}$ of the \"unfreeing transform\" $A_{free} = A_{free}(A_{freeflat})$ is provided by ``paragami`` as a function of the *folded* parameter.  Following standard notation for Jacobian matrices, the rows correspond to $A_{flat}$, the output of the unfreeing transform, and the columns correspond to $A_{freeflat}$, the input to the unfreeing transform.\n",
    "\n",
    "By default this Jacobian matrix is sparse (in large problems, most flat parameters are independent of most free flat parameters), but a dense matrix is fine in this small problem, so we use ``sparse=False``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the Jacobian matrix is (9, 6).\n"
     ]
    }
   ],
   "source": [
    "freeing_jac = a_pattern.unfreeing_jacobian(optimal_a, sparse=False)\n",
    "print(\"The shape of the Jacobian matrix is {}.\".format(freeing_jac.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plug in to estimate the covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the covariance matrix is (9, 9).\n"
     ]
    }
   ],
   "source": [
    "# Estimate the covariance of the flattened value using the Hessian at the optimum.\n",
    "a_flattened_cov = -1 * freeing_jac @ np.linalg.solve(fisher_info, freeing_jac.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Cautionary Note on Using the Fisher Information With Constrained Variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the estimated covariance is rank-deficient.  This is expected, since, for example, $A_{12}$ and $A_{21}$ cannot vary independently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the covariance matrix is (9, 9).\n",
      "The rank of the covariance matrix is 6.\n"
     ]
    }
   ],
   "source": [
    "print('The shape of the covariance matrix is {}.'.format(a_flattened_cov.shape))\n",
    "print('The rank of the covariance matrix is {}.'.format(np.linalg.matrix_rank(a_flattened_cov)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we had erronously defined the function $\\ell_{flat}(A_{flat})$ and tried to estimate the covariance of $A$ using the Hessian of $\\ell_{flat}$.  Then the resulting Hessian would have been *full rank*, because the loss function ``get_loss`` does not enforce the constraint that $A$ be symmetric.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of an erroneous use of Fisher information!\n",
      "The shape of the erroneous covariance matrix is (9, 9).\n",
      "The rank of the erroneous covariance matrix is 9.\n"
     ]
    }
   ],
   "source": [
    "print('An example of an erroneous use of Fisher information!')\n",
    "get_flat_loss = paragami.FlattenedFunction(\n",
    "    original_fun=get_loss, patterns=a_pattern, free=False, argnums=1)\n",
    "get_flat_loss_hessian = autograd.hessian(get_flat_loss, argnum=1)\n",
    "a_flat_opt = a_pattern.flatten(optimal_a, free=False)\n",
    "bad_fisher_info = get_flat_loss_hessian(x, a_flat_opt)\n",
    "\n",
    "bad_a_flattened_cov = -1 * np.linalg.inv(bad_fisher_info)\n",
    "\n",
    "print('The shape of the erroneous covariance matrix is {}.'.format(bad_a_flattened_cov.shape))\n",
    "print('The rank of the erroneous covariance matrix is {}.'.format(np.linalg.matrix_rank(bad_a_flattened_cov)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, we are not justified using the Hessian of $\\ell_{flat}$ to estimate the covariance of its optimizer because the optimum is not \"interior\" -- that is, the argument $A_{flat}$ cannot take legal values in a neighborhood of the optimum, since such values may not be valid covariance matrices.  Overcoming this difficulty is a key advantage of using unconstrained parameterizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting and Checking the Result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shape of $\\mathrm{Cov}(A_{flat})$ is inconvenient because it's not obvious visually which entry of the flattened vector corresponds to which element of $A$.  Again, we can use folding to put the estimated marginal standard deviations in a readable shape.\n",
    "\n",
    "Because the result is not a valid covariance matrix, and we are just using the pattern for its shape, we set `validate` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The marginal standard deviations of the elements of A:\n",
      "[[0.04779966 0.0450593  0.05612339]\n",
      " [0.0450593  0.08469936 0.07468698]\n",
      " [0.05612339 0.07468698 0.13169369]]\n"
     ]
    }
   ],
   "source": [
    "a_pattern.verify = False\n",
    "a_sd = a_pattern.fold(np.sqrt(np.diag(a_flattened_cov)), free=False, validate=False)\n",
    "print('The marginal standard deviations of the elements of A:\\n{}'.format(a_sd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we can compare this estimated covariance with the variability incurred by drawing new datasets and re-optimizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sims = 100\n",
    "optimal_a_draws = np.empty((num_sims, ) + true_a.shape)\n",
    "for sim in range(num_sims):\n",
    "    new_x = draw_data(num_obs, true_a)\n",
    "    new_loss_opt = get_optimum(new_x)\n",
    "    optimal_a_draws[sim] = a_pattern.fold(new_loss_opt.x, free=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual standard deviation:\n",
      "[[0.04415904 0.04753253 0.05444108]\n",
      " [0.04753253 0.08424477 0.08082555]\n",
      " [0.05444108 0.08082555 0.14181567]]\n",
      "Estimated standard deviation:\n",
      "[[0.04779966 0.0450593  0.05612339]\n",
      " [0.0450593  0.08469936 0.07468698]\n",
      " [0.05612339 0.07468698 0.13169369]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a_sd_monte_carlo = np.std(optimal_a_draws, axis=0)\n",
    "\n",
    "plt.plot(a_sd_monte_carlo.flatten(), a_sd.flatten(), 'r.')\n",
    "plt.plot(a_sd_monte_carlo.flatten(), a_sd_monte_carlo.flatten(), 'k')\n",
    "plt.xlabel('Monte Carlo standard deviation')\n",
    "plt.ylabel('Fisher information +\\ndelta method standard deviation')\n",
    "plt.title('Comparision of estimated and exact standard deviations')\n",
    "\n",
    "print('Actual standard deviation:\\n{}'.format(a_sd_monte_carlo))\n",
    "print('Estimated standard deviation:\\n{}'.format(a_sd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
