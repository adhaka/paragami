{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood and Weight Sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autograd\n",
    "from autograd import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "# This contains functions that are used in several paragami examples.\n",
    "import example_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import paragami\n",
    "\n",
    "# Use the original scipy for functions we don't need to differentiate.\n",
    "import scipy as osp\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration, let's consider a simple example: a Gaussian maximum likelihood estimator.\n",
    "\n",
    "$$\n",
    "x_n \\overset{iid}\\sim \\mathcal{N}(\\mu, \\Sigma)\\textrm{, for }n=1,...,N.\n",
    "$$\n",
    "\n",
    "The \"model parameters\" are $\\mu$ and $\\Sigma$, and we will estimate them using maximum likelihood estimation (MLE). Let the data be denoted by $X = (x_1, ..., x_N)$.  For a given set of data weights $W = (w_1, ..., w_N)$, we can define the loss\n",
    "\n",
    "$$\n",
    "\\ell(X, W, \\mu, \\Sigma) = \\frac{1}{2}\\sum_{n=1}^N w_n \\left((x_n - \\mu)^T \\Sigma^{-1} (x_n - \\mu) + \\log |\\Sigma| \\right).\n",
    "$$\n",
    "\n",
    "The loss on the original dataset is given when $W_1=(1,...,1)$, so we will take the MLE to be\n",
    "\n",
    "$$\n",
    "\\hat\\mu, \\hat\\Sigma = \\mathrm{argmax} \\ell(X, W_1, \\mu, \\Sigma).\n",
    "$$\n",
    "\n",
    "We will consider approximating the effect of varying $W$ on the optimal value in the sensitivity section below.\n",
    "\n",
    "Of course, this example has a closed-form optimum as a function of the weights, $W$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat\\mu(W) = \\frac{1}{\\sum_{n=1}^N w_n} \\sum_{n=1}^N w_n x_n \\quad\\quad\\quad\n",
    "\\hat\\Sigma(W) = \\frac{1}{\\sum_{n=1}^N w_n} \n",
    "    \\sum_{n=1}^N w_n \\left(x_n - \\hat\\mu(W) \\right) \\left(x_n - \\hat\\mu(W) \\right)^T\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "However, for expository purposes let us treat it as a generic optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Parameters and Draw Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_obs = 1000\n",
    "\n",
    "# True values of parameters\n",
    "true_sigma = \\\n",
    "    np.eye(3) * np.diag(np.array([1, 2, 3])) + \\\n",
    "    np.random.random((3, 3)) * 0.1\n",
    "true_sigma = 0.5 * (true_sigma + true_sigma.T)\n",
    "\n",
    "true_mu = np.array([0, 1, 2])\n",
    "\n",
    "# Data\n",
    "x = np.random.multivariate_normal(\n",
    "    mean=true_mu, cov=true_sigma, size=(num_obs, ))\n",
    "\n",
    "# Original weights.\n",
    "original_weights = np.ones(num_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out the log likelihood and use it to specify a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at true parameter: 2392.751922600241\n"
     ]
    }
   ],
   "source": [
    "# The loss function is the weighted negative of the log likelihood.\n",
    "def get_loss(norm_param_dict, x, weights):\n",
    "    return np.sum(\n",
    "        -1 * weights * example_utils.get_normal_log_prob(\n",
    "            x, norm_param_dict['sigma'], norm_param_dict['mu']))\n",
    "\n",
    "true_norm_param_dict = dict()\n",
    "true_norm_param_dict['sigma'] = true_sigma\n",
    "true_norm_param_dict['mu'] = true_mu\n",
    "\n",
    "print('Loss at true parameter: {}'.format(\n",
    "    get_loss(true_norm_param_dict, x, original_weights)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten and Fold for Optimization.\n",
    "\n",
    "Note that we have written our loss, `get_loss` as a function of a *dictionary of parameters*, ``norm_param_dict``.\n",
    "\n",
    "We can use `paragami` to convert such a dictionary to and from a flat, unconstrained parameterization for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a `paragami` pattern that matches the input to `get_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_param_pattern = paragami.PatternDict()\n",
    "norm_param_pattern['sigma'] = paragami.PSDSymmetricMatrixPattern(size=3)\n",
    "norm_param_pattern['mu'] = paragami.NumericArrayPattern(shape=(3, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Flatten\" the dictionary into an unconstrained vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flat parameter has shape: (9,)\n"
     ]
    }
   ],
   "source": [
    "norm_param_freeflat = norm_param_pattern.flatten(true_norm_param_dict, free=True)\n",
    "print('The flat parameter has shape: {}'.format(norm_param_freeflat.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize using ``autograd``.\n",
    "\n",
    "We can use this flat parameter to optimize the likelihood directly without worrying about the PSD constraint on $\\Sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Next, wrap the loss to be a function of the flat parameter.\n",
      "Finally, use the flattened function to optimize with autograd.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nNext, wrap the loss to be a function of the flat parameter.')\n",
    "\n",
    "# Later it will be useful to change the weights used for optimization.\n",
    "optim_weights = original_weights\n",
    "get_freeflat_loss = paragami.FlattenFunctionInput(\n",
    "    original_fun=lambda param_dict: get_loss(param_dict, x, optim_weights),\n",
    "    patterns=norm_param_pattern,\n",
    "    free=True)\n",
    "\n",
    "print('Finally, use the flattened function to optimize with autograd.\\n')\n",
    "get_freeflat_loss_grad = autograd.grad(get_freeflat_loss)\n",
    "get_freeflat_loss_hessian = autograd.hessian(get_freeflat_loss)\n",
    "\n",
    "# Initialize with zeros.\n",
    "init_param = np.zeros(norm_param_pattern.flat_length(free=True))\n",
    "mle_opt = osp.optimize.minimize(\n",
    "    method='trust-ncg',\n",
    "    x0=init_param,\n",
    "    fun=get_freeflat_loss,\n",
    "    jac=get_freeflat_loss_grad,\n",
    "    hess=get_freeflat_loss_hessian,\n",
    "    options={'gtol': 1e-12, 'disp': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Fold\" to inspect the result.\n",
    "\n",
    "We can now \"fold\" the optimum back into its original shape for inspection and further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parmeter sigma\n",
      "Optimal:\n",
      "[[ 1.06683522  0.07910048  0.04229475]\n",
      " [ 0.07910048  1.89297797 -0.02650233]\n",
      " [ 0.04229475 -0.02650233  2.92376984]]\n",
      "\n",
      "True:\n",
      "[[1.03745401 0.07746864 0.03950388]\n",
      " [0.07746864 2.01560186 0.05110853]\n",
      " [0.03950388 0.05110853 3.0601115 ]]\n",
      "\n",
      "\n",
      "Parmeter mu\n",
      "Optimal:\n",
      "[-0.04469438  1.03094019  1.8551187 ]\n",
      "\n",
      "True:\n",
      "[0 1 2]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "norm_param_flat0 = copy.deepcopy(mle_opt.x)\n",
    "norm_param_opt = norm_param_pattern.fold(mle_opt.x, free=True)\n",
    "\n",
    "for param in ['sigma', 'mu']:\n",
    "    print('Parmeter {}\\nOptimal:\\n{}\\n\\nTrue:\\n{}\\n\\n'.format(\n",
    "        param, norm_param_opt[param], true_norm_param_dict[param]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis for Approximate LOO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are interested in how our estimator would change if we left out one datapoint at a time.  The leave-one-out (LOO) estimator for the $n^{th}$ datapoint is given by $W_{(n)}=(1,...,1, 0, 1, ..., 1)$, where the $0$ occurs in the $n^{th}$ place, and \n",
    "\n",
    "$$\n",
    "\\hat\\mu_{(n)}, \\hat\\Sigma_{(n)} = \\mathrm{argmax} \\ell(X, W_{(n)}, \\mu, \\Sigma).\n",
    "$$\n",
    "\n",
    "In full generality, one must re-optimize to get $\\hat\\mu_{(n)}, \\hat\\Sigma_{(n)}$.  This can be expensive.  To avoid the cost of repeatedly re-optimizing, we can form a linear approximation to the dependence of the optimal model parameters on the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify a flattented objective function that also depends on the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_freeflat_hyper_loss = paragami.FlattenFunctionInput(\n",
    "    original_fun=lambda param_dict, weights: get_loss(param_dict, x, weights),\n",
    "    patterns=norm_param_pattern,\n",
    "    free=True,\n",
    "    argnums=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a ``HyperparameterSensitivityLinearApproximation`` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_sens = \\\n",
    "    paragami.HyperparameterSensitivityLinearApproximation(\n",
    "        objective_fun=           get_freeflat_hyper_loss,\n",
    "        opt_par_value=           norm_param_flat0,\n",
    "        hyper_par_value=  original_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use ``weight_sens.predict_opt_par_from_hyper_par`` to predict the effect of different weights on the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate the effect of leaving out observation 10.\n",
      "Parameter  sigma\n",
      "Approximate LOO:\n",
      "[[ 1.06789931  0.07906974  0.04205564]\n",
      " [ 0.07906974  1.89118719 -0.0359618 ]\n",
      " [ 0.04205564 -0.0359618   2.90264779]]\n",
      "Original optimum:\n",
      "[[ 1.06683522  0.07910048  0.04229475]\n",
      " [ 0.07910048  1.89297797 -0.02650233]\n",
      " [ 0.04229475 -0.02650233  2.92376984]]\n",
      "\n",
      "\n",
      "Parameter  mu\n",
      "Approximate LOO:\n",
      "[-0.04475159  1.02902066  1.85020216]\n",
      "Original optimum:\n",
      "[-0.04469438  1.03094019  1.8551187 ]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_loo_weight(n):\n",
    "    weights = np.ones(num_obs)\n",
    "    weights[n] = 0\n",
    "    return weights\n",
    "\n",
    "n_loo = 10\n",
    "print('Approximate the effect of leaving out observation {}.'.format(n_loo))\n",
    "loo_weights = get_loo_weight(n_loo)\n",
    "norm_param_flat1 = \\\n",
    "    weight_sens.predict_opt_par_from_hyper_par(loo_weights)\n",
    "approx_loo_norm_param_opt = norm_param_pattern.fold(norm_param_flat1, free=True)\n",
    "\n",
    "for param in ['sigma', 'mu']:\n",
    "    print('Parameter  {}\\nApproximate LOO:\\n{}\\nOriginal optimum:\\n{}\\n\\n'.format(\n",
    "          param, approx_loo_norm_param_opt[param], norm_param_opt[param]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approximation ``predict_opt_par_from_hyper_par`` is fast, so we can easily approximate LOO estimators for each datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_loo_params = []\n",
    "tic = time.time()\n",
    "for n_loo in range(num_obs):\n",
    "    loo_weights = get_loo_weight(n_loo)\n",
    "    norm_par_flat1 = \\\n",
    "        weight_sens.predict_opt_par_from_hyper_par(loo_weights)\n",
    "    approx_loo_params.append(norm_param_pattern.fold(norm_par_flat1, free=True))\n",
    "approx_loo_time_per_n = (time.time() - tic) / num_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with the re-optimizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the exact optimum for a few datapoints to compare with the approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_opt = 20\n",
    "\n",
    "opt_loo_params = []\n",
    "opt_n = np.arange(num_opt)\n",
    "tic = time.time()\n",
    "for n_loo in opt_n:\n",
    "    loo_weights = get_loo_weight(n_loo)\n",
    "    optim_weights = loo_weights \n",
    "    # Start at the previous optimum to speed up optimization.\n",
    "    # Note that you generally need an accurate optimum to measure\n",
    "    # the effect of small changes.\n",
    "    loo_mle_opt = osp.optimize.minimize(\n",
    "        method='trust-ncg',\n",
    "        x0=mle_opt.x,\n",
    "        fun=get_freeflat_loss,\n",
    "        jac=get_freeflat_loss_grad,\n",
    "        hess=get_freeflat_loss_hessian,\n",
    "        options={'gtol': 1e-12, 'disp': False})\n",
    "    opt_loo_params.append(norm_param_pattern.fold(loo_mle_opt.x, free=True))\n",
    "opt_loo_time_per_n = (time.time() - tic) / num_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approximate LOO estimator is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate LOO time per datapoint:\t0.00002 seconds\n",
      "Re-optimization LOO time per datapoint:\t0.14066 seconds\n"
     ]
    }
   ],
   "source": [
    "print('Approximate LOO time per datapoint:\\t{0:.5f} seconds'.format(approx_loo_time_per_n))\n",
    "print('Re-optimization LOO time per datapoint:\\t{0:.5f} seconds'.format(opt_loo_time_per_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quantity one might consider for LOO analysis is the excess loss on the left-out datapoint.\n",
    "\n",
    "$$\n",
    "\\textrm{Excess loss}_n = \\ell(x_n, 1, \\hat\\mu, \\hat\\Sigma) - \\ell(x_n, 1, \\hat\\mu_{(n)}, \\hat\\Sigma_{(n)}).\n",
    "$$\n",
    "\n",
    "We can compare the excess loss as estimated with the approximation and by re-optimizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f8764237d97e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mopt_loo_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mobs_n_excess_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_loo_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopt_n\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mapprox_loo_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mobs_n_excess_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapprox_loo_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopt_n\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_loo_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_loo_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'k'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-f8764237d97e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mopt_loo_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mobs_n_excess_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_loo_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopt_n\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mapprox_loo_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mobs_n_excess_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapprox_loo_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopt_n\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_loo_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_loo_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'k'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-f8764237d97e>\u001b[0m in \u001b[0;36mobs_n_excess_loss\u001b[0;34m(norm_param, n)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mobs_n_excess_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_param_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-c59a8fc4e83c>\u001b[0m in \u001b[0;36mget_loss\u001b[0;34m(norm_param_dict, x, weights)\u001b[0m\n\u001b[1;32m      3\u001b[0m     return np.sum(\n\u001b[1;32m      4\u001b[0m         -1 * weights * example_utils.get_normal_log_prob(\n\u001b[0;32m----> 5\u001b[0;31m             x, norm_param_dict['sigma'], norm_param_dict['mu']))\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrue_norm_param_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "def obs_n_excess_loss(norm_param, n):\n",
    "    return \\\n",
    "        get_loss(norm_param, x[n, :], 1) - \\\n",
    "        get_loss(norm_param_opt, x[n, :], 1)\n",
    "\n",
    "opt_loo_loss = [ obs_n_excess_loss(opt_loo_params[n], n) for n in opt_n ]\n",
    "approx_loo_loss = [ obs_n_excess_loss(approx_loo_params[n], n) for n in opt_n ]\n",
    "\n",
    "plt.plot(opt_loo_loss, opt_loo_loss, 'k')\n",
    "plt.plot(opt_loo_loss, approx_loo_loss, 'ro', markersize=8)\n",
    "plt.xlabel('Excess loss from re-optimizing')\n",
    "plt.ylabel('Approximate excess loss\\nfrom the paragami approximation')\n",
    "plt.title('Approximate vs Exact LOO loss\\n(solid line is y=x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast tools lead to fun exploratory data analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can graph LOO sensitivity of various parameters vs $x_{1n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_1_loo = [ param['mu'][0] for param in approx_loo_params ]  \n",
    "mu_2_loo = [ param['mu'][1] for param in approx_loo_params ]  \n",
    "sigma_12_loo = [ param['sigma'][0, 1] for param in approx_loo_params ]  \n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x[:, 0], mu_1_loo, 'k.')\n",
    "plt.title('mu_1 vs x_{1n}')\n",
    "plt.xlabel('x_{1n}')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x[:, 0], mu_2_loo, 'k.')\n",
    "plt.title('mu_2 vs x_{1n}')\n",
    "plt.xlabel('x_{1n}')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(x[:, 0], sigma_12_loo, 'k.')\n",
    "plt.title('sigma_{12} vs x_{1n}')\n",
    "plt.xlabel('x_{1n}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the excess loss versus the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_n_loss(norm_param, n):\n",
    "    return \\\n",
    "        get_loss(norm_param, x[n, :], 1) - \\\n",
    "        get_loss(norm_param_opt, x[n, :], 1)\n",
    "\n",
    "loo_loss = [ obs_n_loss(approx_loo_params[n], n) for n in range(num_obs) ]\n",
    "\n",
    "# Plot the LOO loss versus each dimension of the data.\n",
    "plt.figure(figsize=(15, 5))\n",
    "for dim in range(3):\n",
    "    plt.subplot(1, 3, dim + 1)\n",
    "    plt.plot(x[:, dim], loo_loss, 'k.')\n",
    "    plt.title('loss vs x_{{{}n}}'.format(dim + 1))\n",
    "    plt.xlabel('x_{{{}n}}'.format(dim + 1))\n",
    "    plt.ylabel('LOO loss')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
