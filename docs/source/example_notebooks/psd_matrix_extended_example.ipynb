{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Example With Covariance Matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autograd\n",
    "from autograd import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import paragami\n",
    "\n",
    "# Use the original scipy (\"osp\") for functions we don't need to differentiate.\n",
    "# When using scipy functions in functions that are passed to autograd,\n",
    "# use autograd.scipy instead.\n",
    "import scipy as osp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will consider flattening and folding a simple symmetric positive semi-definite matrix:\n",
    "\n",
    "$$\n",
    "A = \\left[\n",
    "\\begin{matrix}\n",
    "a_{11} & a_{12} & a_{13}  \\\\\n",
    "a_{21} & a_{22} & a_{23}  \\\\\n",
    "a_{31} & a_{32} & a_{33}  \\\\\n",
    "\\end{matrix}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "Of course, symmetry and positive semi-definiteness impose constraints on the entries $a_{ij}$ of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening and Folding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the Original Space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first consider how to represent $A$ as a vector, which we call simply *flattening*, and then as an unconstrained vector, which we call *free flattening*.\n",
    "\n",
    "When a parameter is flattened, it is simply re-shaped as a vector.  Every number that was in the original parameter will occur exactly once in the flattened shape.  (In the present case of a matrix, this is exactly the same as ``np.flatten``.)\n",
    "\n",
    "$$\n",
    "A = \\left[\n",
    "\\begin{matrix}\n",
    "a_{11} & a_{12} & a_{13}  \\\\\n",
    "a_{21} & a_{22} & a_{23}  \\\\\n",
    "a_{31} & a_{32} & a_{33}  \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\xrightarrow{flatten}\n",
    "A_{flat} = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "a_{flat,1} \\\\\n",
    "a_{flat,2} \\\\\n",
    "a_{flat,3} \\\\\n",
    "a_{flat,4} \\\\\n",
    "a_{flat,5} \\\\\n",
    "a_{flat,6} \\\\\n",
    "a_{flat,7} \\\\\n",
    "a_{flat,8} \\\\\n",
    "a_{flat,9} \\\\\n",
    "\\end{matrix}\\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "a_{11} \\\\\n",
    "a_{12} \\\\\n",
    "a_{13} \\\\\n",
    "a_{21} \\\\\n",
    "a_{22} \\\\\n",
    "a_{23} \\\\\n",
    "a_{31} \\\\\n",
    "a_{32} \\\\\n",
    "a_{33} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to and from $A$ and $A_{flat}$ can be done with the `flatten` method of a `paragami.PSDSymmetricMatrixPattern` pattern.  \n",
    "\n",
    "For the moment, because we are flattening, not free flattening, we use the option `free=False`.  We will discuss the `free=True` option shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A sample positive semi-definite matrix.\n",
    "a = np.eye(3) + np.random.random((3, 3))\n",
    "a = 0.5 * (a + a.T)\n",
    "\n",
    "# Define a pattern and fold.\n",
    "a_pattern = paragami.PSDSymmetricMatrixPattern(size=3)\n",
    "a_flat = a_pattern.flatten(a, free=False)\n",
    "\n",
    "print('Now, a_flat contains the elements of a exactly as shown in the formula above.\\n')\n",
    "print('a:\\n{}\\n'.format(a))\n",
    "print('a_flat:\\n{}\\n'.format(a_flat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also convert from $A_{flat}$ back to $A$ by 'folding'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Folding the flattened value recovers the original matrix.\\n')\n",
    "a_fold = a_pattern.fold(a_flat, free=False)\n",
    "print('a:\\n{}\\n'.format(a))\n",
    "print('a_fold:\\n{}\\n'.format(a_fold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, flattening and folding perform checks to make sure the result is a valid instance of the parameter type -- in this case, a symmetric positive definite matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('The diagonal of a positive semi-definite matrix must not be less',\n",
    "      'than 0, and folding checks this when validate=True, which it is by default:\\n')\n",
    "a_flat_bad = np.array([-1, 0, 0,  0, 0, 0,  0, 0, 0])\n",
    "print('A bad folded value: {}'.format(a_flat_bad))\n",
    "try:\n",
    "    a_fold_bad = a_pattern.fold(a_flat_bad, free=False)\n",
    "except ValueError as err:\n",
    "    print('Folding with a_pattern raised the following ValueError:\\n{}'.format(err))\n",
    "\n",
    "print('\\nIf validate is false, folding will produce an invalid matrix without an error:\\n')\n",
    "a_fold_bad = a_pattern.fold(a_flat_bad, free=False, validate=False)\n",
    "print('Folding a non-pd matrix with validate=False:\\n{}'.format(a_fold_bad))\n",
    "\n",
    "print('\\nHowever, it will not produce a matrix of the wrong shape even when validate is False:\\n')\n",
    "a_flat_very_bad = np.array([1, 0, 0])\n",
    "print('A very bad folded value: {}.'.format(a_flat_very_bad))\n",
    "try:\n",
    "    a_fold_very_bad = a_pattern.fold(a_flat_very_bad, free=False, validate=False)\n",
    "except ValueError as err:\n",
    "    print('Folding with a_pattern raised the following ValueError:\\n{}'.format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In an Unconstrained Space: \"Free\" Flattening and Folding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary flattening converts a 3x3 symmetric PSD matrix into a 9-d vector.  However, as seen above, not every 9-d vector is a valid 3x3 symmetric positive definite matrix.  It is useful to have an \"free\" flattened representation of a parameter, where every finite value of the free flattened vector corresponds is guaranteed valid.\n",
    "\n",
    "To accomplish this for a symmetric positive definite matrix, we consider the Cholesky decomposition $A_{chol}$. This is an lower-triangular matrix with positive diagonal entries such that $A = A_{chol} A_{chol}^T$.  By taking the log of the diagonal of $A_{chol}$ and stacking the non-zero entries, we can construct a 6-d vector, every value of which corresponds to a symmetric PSD matrix.\n",
    "\n",
    "$$\n",
    "% A \\xrightarrow{\\textrm{free flatten}} A_{freeflat} \\quad\\quad \\textrm{where} \\\\\n",
    "A \\xrightarrow{}\n",
    "A_{chol} = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\alpha_{11} & 0 & 0  \\\\\n",
    "\\alpha_{21} & \\alpha_{22} & 0  \\\\\n",
    "\\alpha_{31} & \\alpha_{32} & \\alpha_{33}  \\\\\n",
    "\\end{matrix}\n",
    "\\right] \\xrightarrow{}\n",
    "A_{freeflat} =\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\log(\\alpha_{11}) \\\\\n",
    "\\alpha_{21} \\\\\n",
    "\\alpha_{31} \\\\\n",
    "\\log(\\alpha_{22})\\\\\n",
    "\\alpha_{32} \\\\\n",
    "\\log(\\alpha_{33})\n",
    "\\end{matrix}\n",
    "\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The details of the freeing transform aren't important to the end user, as `paragami` takes care of the transformation behind the scenes with the option `free=True`.  We denote the flattened $A$ in the free parameterization as $A_{freeflat}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('The free flat value a_freeflat is not immediately recognizable as a.\\n')\n",
    "a_freeflat = a_pattern.flatten(a, free=True)\n",
    "print('a:\\n{}\\n'.format(a))\n",
    "print('a_freeflat:\\n{}\\n'.format(a_freeflat))\n",
    "\n",
    "print('However, it transforms correctly back to a when folded.\\n')\n",
    "a_freefold = a_pattern.fold(a_freeflat, free=True)\n",
    "print('a_fold:\\n{}\\n'.format(a_freefold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any length-six vector will free fold back to a valid PSD matrix up to floating point error.  Let's draw 100 random vectors, fold them, and check that this is true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Draw random free vectors and confirm that they are positive semi definite.\n",
    "def assert_is_pd(mat):\n",
    "    eigvals = np.linalg.eigvals(mat)\n",
    "    assert np.min(eigvals) >= -1e-8\n",
    "for draw in range(100):\n",
    "    a_rand_freeflat = np.random.normal(scale=2, size=(6, ))\n",
    "    a_rand_fold = a_pattern.fold(a_rand_freeflat, free=True)\n",
    "    assert_is_pd(a_rand_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Flattening and Folding for Optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Example Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are interested in optimizing some function of $A$, say, a normal model in which the data $x_n \\sim \\mathcal{N}(0, A)$.  Specifically, Let the data be $X = \\left(x_1, ..., x_N\\right)$, where $x_n \\in \\mathbb{R}^3$, and write a loss function as\n",
    "\n",
    "$$\n",
    "\\ell\\left(X, A\\right) =\n",
    "    -\\sum_{n=1}^N \\log P(x_n | A) =\n",
    "    \\frac{1}{2}\\sum_{n=1}^N \\left(x_n^T A^{-1} x_n - \\log|A|\\right) \n",
    "$$\n",
    "\n",
    "Let's simulate some data under this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_obs = 1000\n",
    "\n",
    "# True value of A\n",
    "true_a = np.eye(3) * np.diag(np.array([1, 2, 3])) + np.random.random((3, 3)) * 0.1\n",
    "true_a = 0.5 * (true_a + true_a.T)\n",
    "\n",
    "# Data\n",
    "def draw_data(num_obs, true_a):\n",
    "    return np.random.multivariate_normal(\n",
    "        mean=np.zeros(3), cov=true_a, size=(num_obs, ))\n",
    "\n",
    "x = draw_data(num_obs, true_a)\n",
    "print('X shape: {}'.format(x.shape))\n",
    "\n",
    "def get_loss(x, a):\n",
    "    num_obs = x.shape[0]\n",
    "    a_inv = np.linalg.inv(a)\n",
    "    a_det_sign, a_log_det = np.linalg.slogdet(a)\n",
    "    assert a_det_sign > 0\n",
    "    return 0.5 * (np.einsum('ni,ij,nj', x, a_inv, x) + num_obs * a_log_det)\n",
    "\n",
    "print('Loss at true parameter: {}'.format(get_loss(x, true_a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ``autograd`` and ``scipy.optimize`` with ``paragami``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to minimize the function `loss` using tools like `scipy.optimize.minimize`.  Standard optimization functions take vectors, not matrices, as input, and often require the vector to take valid values in the entire domain.\n",
    "\n",
    "As-written, our loss function takes a positive definite matrix as an input.  We can wrap the loss as a funciton of the free flattened value using the `paragami.FlattenedFunction` class.  That is, we want to define a function $\\ell_{freeflat}$ so that\n",
    "\n",
    "$$\n",
    "\\ell_{freeflat}(X, A_{freeflat}) = \\ell(X, A).\n",
    "$$\n",
    "\n",
    "\n",
    "The resulting function can be passed directly to `autograd` and `scipy.optimize`, and we can estimate\n",
    "\n",
    "$$\n",
    "\\hat{A}_{freeflat} := \\mathrm{argmin}_{A_{freeflat}} \\ell_{freeflat}(X, A_{freeflat})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The arguments mean we're flatting the function get_loss, using\n",
    "# the pattern a_pattern, with free parameterization, and the paramater\n",
    "# is the second one (argnums uses 0-indexing like autograd).\n",
    "get_freeflat_loss = paragami.FlattenedFunction(\n",
    "    original_fun=get_loss, patterns=a_pattern, free=True, argnums=1)\n",
    "\n",
    "print('The two losses are the same when evalated on the folded and flat values:\\n')\n",
    "print('Original loss:\\t\\t{}'.format(get_loss(x, true_a)))\n",
    "true_a_freeflat = a_pattern.flatten(true_a, free=True)\n",
    "print('Free-flattened loss: \\t{}'.format(\n",
    "    get_freeflat_loss(x, true_a_freeflat)))\n",
    "\n",
    "print('\\nNow, use the flattened function to optimize with autograd.\\n')\n",
    "\n",
    "get_freeflat_loss_grad = autograd.grad(get_freeflat_loss, argnum=1)\n",
    "get_freeflat_loss_hessian = autograd.hessian(get_freeflat_loss, argnum=1)\n",
    "\n",
    "def get_optimum(x):\n",
    "    loss_opt = osp.optimize.minimize(\n",
    "        method='trust-ncg',\n",
    "        x0=np.zeros(a_pattern.flat_length(free=True)),\n",
    "        fun=lambda par: get_freeflat_loss(x, par),\n",
    "        jac=lambda par: get_freeflat_loss_grad(x, par),\n",
    "        hess=lambda par: get_freeflat_loss_hessian(x, par),\n",
    "        options={'gtol': 1e-8, 'disp': False})\n",
    "    return loss_opt\n",
    "\n",
    "loss_opt = get_optimum(x)\n",
    "print('Optimization successful: {}\\nOptimal value: {}'.format(\n",
    "    loss_opt.success, loss_opt.fun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization was in the free flattened space, so to get the optimal value of $A$ we must fold it.   We can see that the optimal value is close to the true value of $A$, though it differs due to randomness in $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimal_freeflat_a = loss_opt.x\n",
    "optimal_a = a_pattern.fold(optimal_freeflat_a, free=True)\n",
    "print('True a:\\n{}\\n\\nOptimal a:\\n{}'.format(true_a, optimal_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Flattening and Folding with the Fisher Information for Frequentist Uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher Information and the Delta Method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wanted to use the Hessian of the objective (the observed Fisher information) to estimate a frequentist confidence region for $A$.  In standard notation, covariance is of a vector, so we can write what we want in terms of $A_{flat}$ as $\\mathrm{Cov}(A_{flat})$.\n",
    "The covariance between two elements of $A_{flat}$ corresponds to that between two elements of $A$.  For example, using the notation given above,\n",
    "\n",
    "$$\n",
    "\\mathrm{Cov}(a_{flat,1}, a_{flat,2}) = \\mathrm{Cov}(a_{11}, a_{12}) = \\mathrm{Cov}(a_{11}, a_{21})\\\\\n",
    "\\mathrm{Var}(a_{flat,4}) = \\mathrm{Var}(a_{21}) = \\mathrm{Var}(a_{12}),\n",
    "$$\n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use the observed Fisher information of $\\ell_{freeflat}$ and the Delta method to estimate $\\mathrm{Cov}(A_{flat})$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{Cov}(A_{freeflat}) &\\approx\n",
    "-\\left( \\left.\n",
    "\\frac{\\partial^2 \\ell_{freeflat}}{\\partial A_{freeflat} \\partial A_{freeflat}^T}\n",
    "\\right|_{\\hat{A}_{freeflat}} \\right)^{-1}\n",
    "&\\quad\\textrm{(Fisher information)}\n",
    "\\\\\n",
    "\\mathrm{Cov}(A_{free}) &\\approx\n",
    "\\left(\\frac{d A_{free}}{dA_{freeflat}^T}\\right)\n",
    "\\mathrm{Cov}(A_{freeflat})\n",
    "\\left(\\frac{d A_{free}}{dA_{freeflat}^T}\\right)^T\n",
    "&\\quad\\textrm{(Delta method)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hessian required for the covariance can be calculated directly using ``autograd``.  (Note that the loss is the negative of the log likelihood.)  The shape is, of course, the size of $A_{freeflat}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fisher_info = -1 * get_freeflat_loss_hessian(x, loss_opt.x)\n",
    "print(\"The shape of the Fisher information amtrix is {}.\".format(fisher_info.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jacobian matrix $\\frac{d A_{free}}{dA_{freeflat}^T}$ of the \"unfreeing transform\" $A_{free} = A_{free}(A_{freeflat})$ is provided by ``paragami`` as a function of the *folded* parameter.  Following standard notation for Jacobian matrices, the rows correspond to $A_{flat}$, the output of the unfreeing transform, and the columns correspond to $A_{freeflat}$, the input to the unfreeing transform.\n",
    "\n",
    "By default this Jacobian matrix is sparse (in large problems, most flat parameters are independent of most free flat parameters), but a dense matrix is fine in this small problem, so we use ``sparse=False``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freeing_jac = a_pattern.unfreeing_jacobian(optimal_a, sparse=False)\n",
    "print(\"The shape of the Jacobian matrix is {}.\".format(freeing_jac.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plug in to estimate the covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Estimate the covariance of the flattened value using the Hessian at the optimum.\n",
    "a_flattened_cov = -1 * freeing_jac @ np.linalg.solve(fisher_info, freeing_jac.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Cautionary Note on Using the Fisher Information With Constrained Variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the estimated covariance is rank-deficient.  This is expected, since, for example, $A_{12}$ and $A_{21}$ cannot vary independently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('The shape of the covariance matrix is {}.'.format(a_flattened_cov.shape))\n",
    "print('The rank of the covariance matrix is {}.'.format(np.linalg.matrix_rank(a_flattened_cov)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we had erronously defined the function $\\ell_{flat}(A_{flat})$ and tried to estimate the covariance of $A$ using the Hessian of $\\ell_{flat}$.  Then the resulting Hessian would have been *full rank*, because the loss function ``get_loss`` does not enforce the constraint that $A$ be symmetric.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('An example of an erroneous use of Fisher information!')\n",
    "get_flat_loss = paragami.FlattenedFunction(\n",
    "    original_fun=get_loss, patterns=a_pattern, free=False, argnums=1)\n",
    "get_flat_loss_hessian = autograd.hessian(get_flat_loss, argnum=1)\n",
    "a_flat_opt = a_pattern.flatten(optimal_a, free=False)\n",
    "bad_fisher_info = get_flat_loss_hessian(x, a_flat_opt)\n",
    "\n",
    "bad_a_flattened_cov = -1 * np.linalg.inv(bad_fisher_info)\n",
    "\n",
    "print('The shape of the erroneous covariance matrix is {}.'.format(bad_a_flattened_cov.shape))\n",
    "print('The rank of the erroneous covariance matrix is {}.'.format(np.linalg.matrix_rank(bad_a_flattened_cov)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, we are not justified using the Hessian of $\\ell_{flat}$ to estimate the covariance of its optimizer because the optimum is not \"interior\" -- that is, the argument $A_{flat}$ cannot take legal values in a neighborhood of the optimum, since such values may not be valid covariance matrices.  Overcoming this difficulty is a key advantage of using unconstrained parameterizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting and Checking the Result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shape of $\\mathrm{Cov}(A_{flat})$ is inconvenient because it's not obvious visually which entry of the flattened vector corresponds to which element of $A$.  Again, we can use folding to put the estimated marginal standard deviations in a readable shape.\n",
    "\n",
    "Because the result is not a valid covariance matrix, and we are just using the pattern for its shape, we set `validate` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_pattern.verify = False\n",
    "a_sd = a_pattern.fold(np.sqrt(np.diag(a_flattened_cov)), free=False, validate=False)\n",
    "print('The marginal standard deviations of the elements of A:\\n{}'.format(a_sd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we can compare this estimated covariance with the variability incurred by drawing new datasets and re-optimizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_sims = 50\n",
    "optimal_a_draws = np.empty((num_sims, ) + true_a.shape)\n",
    "for sim in range(num_sims):\n",
    "    new_x = draw_data(num_obs, true_a)\n",
    "    new_loss_opt = get_optimum(new_x)\n",
    "    optimal_a_draws[sim] = a_pattern.fold(new_loss_opt.x, free=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_sd_monte_carlo = np.std(optimal_a_draws, axis=0)\n",
    "\n",
    "plt.plot(a_sd_monte_carlo.flatten(), a_sd.flatten(), 'r.')\n",
    "plt.plot(a_sd_monte_carlo.flatten(), a_sd_monte_carlo.flatten(), 'k')\n",
    "plt.xlabel('Monte Carlo standard deviation')\n",
    "plt.ylabel('Fisher information +\\ndelta method standard deviation')\n",
    "plt.title('Comparision of estimated and exact standard deviations')\n",
    "\n",
    "print('Actual standard deviation:\\n{}'.format(a_sd_monte_carlo))\n",
    "print('Estimated standard deviation:\\n{}'.format(a_sd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
